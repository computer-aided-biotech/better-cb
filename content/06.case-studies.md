## Case Studies

We will now exemplify the use of the aforementioned tools by referring to several published studies in the field, and detailing how these type of projects implement different tools depending on their scope and magnitude. Note that this is a non-comprehensive list, intended as a short overview of how can projects in computational biology benefit from robust computational tools and good software development practices. Additionally, it will be quickly evident by reading these case studies that there is a lot of repetition in the chosen tools. For instance, all projects use an environment manager such as Conda, and a version control system like Git. This repetition is intentional, as it denotes that some tools, especially the ones at the personal research level regarding your project structure and good coding practices, are ubiquitous in computational biology and can benefit any project in the field.

### Case study 1: Variant calling in a large cohort

Nowadays, the availability and affordability of next-generation sequencing (NGS) allows routine sequencing of dozens to even thousands of individuals. These resequencing experiments enable discovery and genotyping of genomic variation within a population to answer key questions regarding population dynamics and susceptibility to disease. For this example, let's consider a project that performed whole-genome Illumina sequencing and called variants in thousands of individuals such as [@10.1101/2021.02.06.430068]. In a project of this nature, the challenge resides on applying a multi-step variant calling pipeline on a big sample size in a reproducible manner.

This project will require to use Bash to interact with an HPC system, given the large amount of data. Additionally, we recommend writing the pipeline using a workflow automation tool like Snakemake to reproduce the pipeline consistently and reliably across samples. Python can be used inside of the Snakefile to parse sample names and perform other data handling operations. The folder structure should follow Snakemake's recommendations. A Conda environment can hold all necessary software since Bioconda contains a wide array of software designed for genomics analyses. The actual coding of the workflow can be done in any text editor that offers easy integration with Git commands and Git hosting repository, such as Atom or Visual Studio Code. Although there are no formal styling rules for Snakefiles, embrace consisting variable and rule naming, and comment your code when needed.

A project of this magnitude will usually imply collaborators from other research groups. The pipelines and scripts can be shared using a GitHub repository. If privacy is a concern, the repository can be set as private and made public in later stages of the project. To write the manuscript, a general-purpose word processors such as Google Doc would suffice, especially if not all your collaborators are familiarized with Git and tools like Manubot. Considering that this type of data are a valuable resource for the community, follow FAIR principles for data sharing. Besides uploading the raw data in a repository like ENA or NCBI, we encourage you to openly share your code, analyses and other types of data in either a GitHub repository or a dedicated research repository like Zenodo.

### Case study 2: scRNA-seq data integration

Single-cell RNA-seq (scRNA-seq) is a rapidly evolving technology that has enable the study of cell heterogeneity and developmental changes of a cell lineage, otherwise intractable with bulk RNA-seq. Current scRNa-seq experiment deliver the transcriptomic profiles of thousand to a million of cells [@doi:10.1038/nprot.2017.149], making them a suitable target for machine or deep learning approaches. Among the many challenges imposed by this technology, a relevant one is the integration of scRNA-seq datasets, especially in case-control studies where cell types need to be functionally matched across dataset before evaluating differences across conditions. For this case study, we will consider the development of an unsupervised deep learning method for data integration as described in [@doi:10.1186/s13059-019-1766-4].

This kind of project often uses a combination of Python, R and Bash scripting depending on the task. Python can be used to write and train deep learning models with TensorFlow and PyTorch libraries, while R enables straightforward pre-processing with tools such as Seurat, and Bash can handle large scale processing of raw data files which cannot be loaded directly into RAM (often required by R and Python applications). Additionally, we advice to use the Pythonâ€™s reticulate library to incorporate Python tools into the existing R ecosystem. To set up your working directory, we recommend a structure like Cookiecutter Data Science, which includes separated folders for trained models and other components of a deep learning project. To set up a software environment, Python virtual environments like pipenv and virtualenv work well with Tensorflow and PyTorch. The actual coding can be done in any general purpose text editor, such as Atom or Visual Studio Code, where updates can be easily push/pull to GitHub. As a good practice, keep the code modular and properly commented, and use detailed filenames with data stamps and model parameters to facilitate revisiting projects. Additionally, take advantage of tools such as TensorFlow's TensorBoard to diagnose, visualize and experiment with your models. 

When working with collaborators, the best way to share the code is through a Git hosting service like GitHub. When multiple-users need to edit the code in real-time, then Google CoLab is an excellent platform since they offer interactive coding and GPU access. In addition to the code repository, a Manubot can be crated to write the manuscript collaboratively. To make your tool accessible to a larger community, the code can be taken from an internal repository to a public GitHub, which must include an overview of the tool and a license file. Considering that most users in the field use R, you can go step further and share your code as Bioconductor package, making sure your method can be called directly in R and that interacts with standard data-structures in the field. For better reproducibility, document your method including example tutorials in a platform like ReadTheDoc, and share the software environment needed to deploy the models as a Docker container. GitHub issues and Bioconductor forums are suitable platforms to promptly reply to users questions, bugs reports, and enhancement opportunities.

### Case study 3: Tool development





