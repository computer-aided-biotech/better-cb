## Case Studies

We will now exemplify the use of the aforementioned tools by referring to several published studies in the field, and detailing how these type of projects implement different tools depending on their scope and magnitude. Note that this is a non-comprehensive list, intended as a short overview of how can projects in computational biology benefit from robust computational tools and good software development practices. Additionally, it will be quickly evident by reading these case studies that there is a lot of repetition in the chosen tools. For instance, all projects use an environment manager such as Conda, and a version control system like Git. This repetition is intentional, as it denotes that some tools, especially the ones at the personal research level regarding your project structure and good coding practices, are ubiquitous in computational biology and can benefit any project in the field.

### Case study 1: Differential gene expression

Differential gene expression (DGE) analysis is a routine research tool in the field of functional genomics. Its main goal is to determine quantitative changes in gene expression levels between different experimental conditions or different populations. Nowadays, given the availability of NGS technologies, most DGE analysis are based on RNA-seq data, being the primary application of the technology [@doi:10.1038/s41576-019-0150-2]. Here, we will consider a study designed to gain insights regarding a specific condition of interest, leading to interest genes that can be functionally characterized in animal models afterwards. The experimental setup included a control group and an experimental group with the condition of interest, both sequenced using RNA-seq. The experiment was conducted four times independently to reach a total of four replicates per group.

In this example, the most relevant part is personal research. Following this work's framework, the first step is to decide which programming languages to use. Considering that the bioinformatics analysis will include a first step performed in the command line—where read will be trimmed and aligned to a reference transcriptome, and the number of reads per transcripts will be counted—followed by a second step for data filtering and statistical analysis in R, the programming languages to use will be Shell and R. The folder structure must have separated spaces for raw data, results, documentation and scripts used in the analysis. We recommend to clone the RR-init template into our HPC since a simple structure will suffice. For this project, the best option will be to use a Conda environment with R installed, where we can download packages from Bioconductor and Bioconda. A bash script written in VIM or Emacs will be a suitable option for the first step in the HPC, and R Studio for the second part of analysis and visualization. We advice to follow literature programming, especially when writing R code, and to track changes using Git.

### Case study 2: scRNA-seq deconvolution



### Case study 3: Genome-scale metabolic model generation

Genome-scale metabolic modeling is a common approach used in systems biology, where the complete metabolic network of a given organism is inferred from the genome annotation and converted to a matrix that contains every reaction's stoichiometry, hence referred to as a stoichiometric matrix. Using a few simple assumptions, this matrix can then be used to perform simulations under different experimental conditions, to obtain additional insight into cellular physiology. As case study we chose the generation, curation and validation of a genome-scale model for _Parageobacillus thermoglucosidasius_ [@doi:10.1101/2021.02.01.429138], a thermophilic facultative anaerobic bacteria with promising traits for industrial metabolic engineering.

The first step in a project of this nature is to use one of the many reconstruction algorithms available [@doi:10.1186/s13059-019-1769-1] to start from what is referred to as a draft reconstruction; therefore, the choice of programming language for that section will depend on the selected algorithm. After that, there is a lengthy step of model curation and gap filling, in order to end up with a model that can produce all necessary building blocks of the cell. For this step we recommend a basic setup of Python as programming language and Conda as environment manager, due to their ease of use and the growing number of Python packages being developed in the field. Additionally, we advice using Jupyter Notebook as main working setup and Git for version control, as you can use different notebooks (or different versions of a notebook) as logs of analysis performed on your working draft. When collaborating, tools that will be especially useful while working on a genome-scale model are unit-testing, to ensure your model maintains a certain quality as you and others develop it [@doi:10.1038/s41587-020-0446-y], and ReviewNB, to keep track of changes in notebooks across commits and/or branches. Finally, when sharing the model in the community, Zenodo is a great option for defining different versions of the model, and any issue tracker will make it possible for users to pinpoint mathematical or biological inconsistencies in the network.

### Case study 4: Variant calling in a human cohort

Nowadays, the availability and affordability of next-generation sequencing (NGS) allows routine sequencing of dozens to even thousands of individuals. These resequencing experiments enable discovery and genotyping of genomic variation within a population to answer key questions regarding population dynamics and susceptibility to disease. For this example, let's consider a project that performed whole-genome Illumina sequencing of hundred of human individuals from a particular population, with the goal to characterize their genetic diversity. Here, the challenge resides on applying a multi-step variant calling pipeline on a big sample size in a reproducible manner.

This projects will need to be run in an HPC given the big file sizes, and therefore Shell scripting will be necessary. Additionally, we recommend writing the pipeline using a workflow automation tool like Snakemake to be able to call variants consistently and reliably across samples. Python code can be used inside of the Snakefile to parse sample names. The folder structure should follow Snakemake's recommendations. A Conda environment can hold all necessary software since Bioconda contains a wide array of software designed for this particular application. The actual coding of the workflow can be done in Vim or Emacs in the HPC clusters. Although there are no formal styling rules for Snakefiles, since it is fundamentally Python code, it can be written following PEP-8 standards. As always, we recommend to track changes by making the scripts folder a Git repository.

A project of this magnitude will usually imply collaborators from other laboratories. The pipelines and scripts can be shared using a GitHub repository. If privacy is a concern at the early stages of the project, the repository can be set as private and collaborators invited from the website. To write the manuscript collaboratively, the best option would be a general-purpose word processors such as Google Doc, because most likely some collaborators will not be familiarized with GitHub and Manubot. When sharing this data with collaborators and later on with the community, remember FAIR principles. We advice to upload the raw data in a data repository like ENA or NCBI, and share your results and analyses openly in either a GitHub repository or a dedicated research repository like Zenodo.

### Case study 5: Tool development

The last (?) case study we will present
