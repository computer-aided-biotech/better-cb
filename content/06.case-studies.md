## Case Studies

We will now exemplify the use of the aforementioned tools by referring to several published studies in the field, and detailing how these type of projects implement different tools depending on their scope and magnitude. Note that this is a non-comprehensive list, intended as a short overview of how can projects in computational biology benefit from robust computational tools and good software development practices. Additionally, it will be quickly evident by reading these case studies that there is a lot of repetition in the chosen tools. For instance, all projects use an environment manager such as Conda, and a version control system like Git. This repetition is intentional, as it denotes that some tools, especially the ones at the personal research level regarding your project structure and good coding practices, are ubiquitous in computational biology and can benefit any project in the field.



### Case study 1: Variant calling in a human cohort

Nowadays, the availability and affordability of next-generation sequencing (NGS) allows routine sequencing of dozens to even thousands of individuals. These resequencing experiments enable discovery and genotyping of genomic variation within a population to answer key questions regarding population dynamics and susceptibility to disease. For this example, let's consider a project that performed whole-genome Illumina sequencing of hundred of human individuals from a particular population, with the goal to characterize their genetic diversity. Here, the challenge resides on applying a multi-step variant calling pipeline on a big sample size in a reproducible manner.

This projects will need to be run in an HPC given the big file sizes, and therefore Shell scripting will be necessary. Additionally, we recommend writing the pipeline using a workflow automation tool like Snakemake to be able to call variants consistently and reliably across samples. Python code can be used inside of the Snakefile to parse sample names. The folder structure should follow Snakemake's recommendations. A Conda environment can hold all necessary software since Bioconda contains a wide array of software designed for this particular application. The actual coding of the workflow can be done in Vim or Emacs in the HPC clusters. Although there are no formal styling rules for Snakefiles, since it is fundamentally Python code, it can be written following PEP-8 standards. As always, we recommend to track changes by making the scripts folder a Git repository.

A project of this magnitude will usually imply collaborators from other laboratories. The pipelines and scripts can be shared using a GitHub repository. If privacy is a concern at the early stages of the project, the repository can be set as private and collaborators invited from the website. To write the manuscript collaboratively, the best option would be a general-purpose word processors such as Google Doc, because most likely some collaborators will not be familiarized with GitHub and Manubot. When sharing this data with collaborators and later on with the community, remember FAIR principles. We advice to upload the raw data in a data repository like ENA or NCBI, and share your results and analyses openly in either a GitHub repository or a dedicated research repository like Zenodo.

### Case study 2: scRNA-seq data integration

Single-cell RNA-seq (scRNA-seq) is a rapidly evolving technology that has enable the study of cell heterogeneity and developmental changes of a cell lineage, otherwise intractable with bulk RNA-seq. Current scRNa-seq experiment deliver the transcriptomic profiles of thousand to a million of cells [@doi:10.1038/nprot.2017.149], and are better analyzed using machine or deep learning tools. Among the many challenges imposed by this technology, a relevant one is the integration of multiple scRNA-seq datasets, especially in case-control studies where cell types need to be functionally matched across dataset before evaluating differences across conditions. For this case study, we will consider the development of an unsupervised deep learning method for data integration as described in [@doi:10.1186/s13059-019-1766-4].

This kind of project often use a combination of Python, R and Bash scripting depending on the task. Python is used to write and train deep learning models with Tensorflow and PyTorch libraries. R enables straightforward pre-processing with tools such as Seurat. Bash is used for large scale processing of raw data files which either cannot be loaded directly into RAM (often required by R and Python). Additionally, because most researchers in this field primarily use R, Pythonâ€™s reticulate library can be used to enable users to call Python directly from R.

The last (?) case study we will present
