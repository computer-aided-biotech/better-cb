## Level 2: Collaboration

Collaboration is a key aspect of scientific research, but it is especially relevant in computational biology, where interdisciplinary knowledge is often needed. Collaborators can take different forms: your boss or advisor, colleagues or lab mates, other laboratories, people from academia or industry, or your future self (as discussed in  [Level 1](#level-1-personal-research)). Although collaborators can have a wide range of involvement with your project, here we will consider they all share a direct relationship with you and your research (contrary to a community, which is an open group of a large number of people, as we will discuss in [Level 3](#level-3-community)). Each type of collaboration requires its own set of good practices, which we will cover in the next paragraphs.

### 2.1 Share code

Sharing code is one of the most common practices in software development, where large teams work together developing complex functions and scripts. Although computational biology projects are usually not as big, proper ways of sharing code are still essential, as it is not desired to have file conflicts as soon as two different researchers change the same piece of code. Hosting services such as GitHub [@https://github.com], GitLab [@https://gitlab.com] and Bitbucket [@https://bitbucket.org] (Table @tbl:collaboration-tools) allow for having a Git repository stored online, by creating a copy of the repository known as the "remote", which becomes the official version of the repository. The key advantage of using a remote is that there will be no direct interaction between different local copies of the repository, also known as "clones", but instead each clone will interact with the remote exclusively, updating it only if no conflicts between the two exist. This way, if a collaborator updated the remote repository, other collaborators will not be able to send their changes until they make sure to update their local copy.

| Goal | Tools |
| --- | ---------- |
| Share code | &bull; _Hosting services_: **GitHub** [@https://github.com], **GitLab** [@https://gitlab.com], **Bitbucket** [@https://bitbucket.org]. <br />&bull; _Git branching strategies:_ **GitHub flow** [@https://guides.github.com/introduction/flow/].<br />&bull; _Tests:_ correctness (e.g. **pytest** [@https://docs.pytest.org/en/stable/]), coverage (e.g. **codecov** [@https://about.codecov.io/]), automation (e.g. **tox** [@https://tox.readthedocs.io/en/latest/], **Travis CI** [@https://travis-ci.com/], **Github Actions** [@https://github.com/features/actions]).<br />&bull; _Code reviews_: **Github** [@https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/reviewing-changes-in-pull-requests], **Crucible** [@https://www.atlassian.com/software/crucible], **Upsource** [@https://www.jetbrains.com/upsource/]. |
| Share data | &bull; _FAIR principles_ [@doi:10.1038/sdata.2016.18]. <br />&bull; _Tidy data_ [@doi:10.18637/jss.v059.i10]. <br />&bull; _Data version control_ [@https://dvc.org/]. |
| Share data science notebooks | &bull; _Static:_ **GitHub**, **GitLab**, **NBviewer** [@https://nbviewer.jupyter.org/]. <br />&bull; _Interactive:_ **Binder** [@https://mybinder.org], **Google CoLab** [@https://colab.research.google.com/].<br />&bull; _Comparative:_ **ReviewNB** [@https://www.reviewnb.com/]. |
| Share workflows | • _General hosting services_: **GitHub**, **GitLab**, **Bitbucket**<br />• _Dedicated workflow repositories:_ **WorkflowHub** [@https://workflowhub.eu/] |
| Share manuscripts | &bull; _General-purpose word processors:_ **Google Docs** [@https://www.google.com/docs/about/], **Office 365** [@https://www.microsoft.com/en-us/microsoft-365].<br />&bull; _Scholarly word processors:_ **Authorea** [@https://www.authorea.com/].<br />&bull; _Online applications supporting Markup Languages:_ **Overleaf** (LaTeX) [@https://www.overleaf.com/], **Manubot** (Markdown + GitHub) [@https://manubot.org/]. |

Table: Tools for collaborative research. {#tbl:collaboration-tools}

To guarantee that different collaborators can work simultaneously in the same repository, a good idea is to implement some type of branching strategy in the repository (Table @tbl:collaboration-tools). In a small team of collaborators, the most common strategy is to have a single `master` branch and generate from it branches that each different developer can work on. Then, whenever the developer is ready, they can request to combine—or "merge"—the changes from their branch into the master branch, in a process known as "pull request" or PR for short. Once a PR has been opened, collaborators can review it and approve it, so that it can be merged into the master branch, which will now include the branch's commits as part of its history. This branching strategy is sometimes referred to as GitHub flow [@https://guides.github.com/introduction/flow/] and will suffice for most projects. For more complex branching systems, see [Level 3](#level-3-community).

Using Git hosting services for collaboration has many additional benefits. The commit history not only shows what was done at each point in time, but also which collaborator did it, so that if, for example, a bug was introduced, commands such as `git blame` will show which collaborator caused it. To ensure hunting down bugs is easy, descriptive commit messages that follow a standard [@https://sparkbox.com/foundry/semantic_commit_messages;@https://www.conventionalcommits.org/en/v1.0.0/] is recommended. Collaborators can also create "forks", i.e. full copies of repositories under their own possession. Git hosting services can be accessed interactively online, or from the terminal with tools such as GitHub CLI [@https://cli.github.com/]. Finally, Git hosting services also allow collaborators to open issues [@https://docs.github.com/en/github/managing-your-work-on-github/about-issues] for listing pending to-do's and/or asking questions, acting as an open forum for development discussions, which has the advantage of remaining accessible for the future (as opposed to closed e-mail discussions). We will consider additional advantages of using Git hosting services, in terms of interacting with a user base, in [Section 3.3](#make-your-research-sustainable).

Another important concept to internalize when developing code, especially together with other collaborators, is to develop unit tests (Table @tbl:collaboration-tools). Unit tests are scripts that will run to determine if specific modules/functions work as intended within the codebase, so that if later the function grows in scope, its proper basic functioning is ensured. For instance, if a function was defined for adding numbers, a simple test would be to asses if the function outputs 13 when the inputs 6 and 7 are provided. Tools such as `pytest` [@https://docs.pytest.org/en/stable/] for Python and `testthat` [@https://testthat.r-lib.org/] for R exist to then detect said scripts, and run all of them to display if any specific section is failing. It is a very good practice to develop tests at the same time you develop code (at the personal research level), as adding tests _a posteriori_ is significantly harder (although sometimes inevitable). It is even better practice to test every single step of the code (from data loading to figure plotting), a concept known in software development as end-to-end testing [@https://smartbear.com/solutions/end-to-end-testing/].

Going beyond testing correctness, tools such as `flake8` [@https://flake8.pycqa.org/en/latest/] will test styling preferences (for complying with PEP8), `safety` [@https://pyup.io/safety/] will test for vulnerabilities among the software's dependencies, and Codecov [@https://about.codecov.io/] will test coverage, or the percentage of the codebase tested (as a rule of thumb for the latter, the more lines of code are tested, the more reliable the software will be). All these different types of tests can be funneled into a single testing pipeline that can run automatically whenever desired. This process is known as Continuous Integration (CI), and can be tuned to run locally whenever commits are made, or online whenever a pull request is opened and/or merged. When running locally, an environment manager / command line tool such as `tox` [@https://tox.readthedocs.io/en/latest/] helps to ensure all tests are executed under different python versions. For setting up the CI cycle online, different CI tools such as Travis CI [@https://travis-ci.com/] or Circle CI [@https://circleci.com/] exist, and more recently GitHub actions [@https://github.com/features/actions] has been introduced for running the integration directly from GitHub.

Having tests is a great way of ensuring our code fulfills a certain level of correctness and styling. However, it is no replacement for a human assessment to determine if the code is correct, necessary, and useful. Therefore, code reviewing is essential whenever developing code in collaboration (Table @tbl:collaboration-tools). Tools such as Crucible [@https://www.atlassian.com/software/crucible] and Upsource [@https://www.jetbrains.com/upsource/] exist for making in-line reviews of each file, but the most common approach is to directly review using the online review tools from the hosting service. In the case of GitHub [@https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/reviewing-changes-in-pull-requests], this not only allows the reviewer to open a comment in any line of the code (which creates a thread for the original author to reply), but also to suggest changes that can be approved or dismissed. When reviewing, there are a series of things to look for (from functionality to documentation), and good practices to keep in mind (such as phrasing the comments in a constructive way), which are outside of the scope of this review but presented in detail elsewhere [@https://google.github.io/eng-practices/review/reviewer/;@https://phauer.com/2018/code-review-guidelines/].

### 2.2 Share data

The practices of sharing data stem from the same place as with sharing code: we should store our dataset and any changes to it in a repository, and ensure it complies with standards by testing its quality. However, due to data having a more consistent structure than code, as they are often processed and outputted by machines in standard formats, there are additional criteria that should be considered when we share it with collaborators (and later on with the community). The main set of guidelines that represent these criteria were outlined a few years ago in what is known as the FAIR principles [@doi:10.1038/sdata.2016.18]: data should be Findable, i.e. easy to find online; Accessible, i.e. easy to access once found; Interoperable, i.e. easy to integrate with other data/applications/workflows/etc.; and Reusable, i.e. presented in a way that allows for others to use it for the same purpose or different settings.

For making data findable, research repositories such as Zenodo [@https://zenodo.org] and Figshare [@https://figshare.com/about] allow you to assign a digital object identifier (DOI) to any group of files you upload, including data and/or code. Alternatively, regular code repositories like GitHub can be used instead, as you can employ commits and/or releases to identify specific versions of the data (see [Section 3.1](#make-your-research-accessible)), in combination with extensions for Large File Storage such as git LFS [@https://git-lfs.github.com/], in the case of data files larger than 100 MB [@https://docs.github.com/en/github/managing-large-files/what-is-my-disk-quota]. A final alternative is the Data Version Control (DVC) initiative [@https://dvc.org/], which is especially useful when doing machine learning, as it can keep track of data, machine learning models, and even scoring metrics.

For making data accessible, we encourage as much as possible to make your repositories open access, but in cases in which you or your collaborators prefer some restrictions, you can create guest accounts to provide access to private repositories. For making data interoperable, distinctions between raw and clean data have been made [@doi:10.1371/journal.pcbi.1000424], with raw data being the files that came out of the measuring device, and clean data the files that are ready to be used for any computational analysis. An important characteristic that clean data should have is to be "tidy", which is reviewed in detail elsewhere [@doi:10.18637/jss.v059.i10]. Finally, for making data reusable, thorough documentation of the data—including experimental design, measurements units and sources of error —is required.

### 2.3 Share data science notebooks

As we previously discussed, Jupyter Notebook have become a fundamental tool of data analysis. Notebooks can be shared with collaborators using either static or interactive options. The former, as the name indicates, shares computational notebooks as a rendered text, written internally in HTML. Static notebooks are a good option when you want to avoid any modifications and can work as an archive of past analyses, but interacting with its content is cumbersome—the file must be downloaded and run in a local Jupyter installation. Git-based code repositories, such as GitHub and GitLab, automatically render notebooks that can be later shared using the GitHub repository URL. To facilitate this process, the Project Jupyter provides a web application called NBviewer, where you can paste a Jupyter Notebook's URL, publicly hosted in GitHub or elsewhere, and renders the file into a static HTML web page with a stable link. 

Interactive notebooks, in the other hand, not only render the file but also allow collaborators to fully interact with it, tinkering parameters or trying new input data—no installation required. The Binder Projects (which is also part of the Project Jupyter) offers the Binder service, where any publicly hosted Git-based repository can be open with a Jupyter Notebook interface. The user can fully interact with any notebook within the repository, although changes will not be saved to the original file. The platforms supports Python and R among other languages, and any additional packages required to run the analysis need to be specified in a configuration file within the repository. Similarly, Jupyter Notebooks can be run interactively using Google CoLaboratory (CoLab), which is available to anyone with a Google account. Notebooks can be updated locally, from any public GitHub repository, or from Google Drive. In both cases, the machines provided by these services are comparable to a modern laptop, hence these tools may not be suitable for computing-intensive tasks.

Sometimes computational biologists need to work and edit a notebook together. In those cases, notebooks need to be treated as any other piece of code: updates from different collaborators must be managed with version control in a platform such as GitHub. The problem, however, is that Git-based hosting services deal with notebooks as if they were HTML text, where changes between versions are hard to visualize. To better compare these changes, there is NBreview, which renders and display in parallel the old and new versions of a notebook for easy comparison. The tool can be easily installed using your GitHub account.

### 2.4 Share computational workflows 

Computational biology projects often demands using multi-step analyses with dozens of third-party software and dependencies. Although these steps can be shared as documentation, complex workflows are better shared as stand-alone code that can be easily run with minimal file manipulation from collaborators. Doing so can safeguard the reproducibility and replicability of the analysis, leading to better science and less issues down the road. 

The simplest way to share a pipeline is to generate a Bash script that receives input files from the command line, thus, allowing to run it with different input data. However, Bash scripts offer little control over the overall workflow and cannot re-run specific parts of the pipeline. To address these issues, pipelines are better shared using a workflow automation system. Theoretically, all the instructions regarding the workflow could be written in the main pipeline file—in Snakemake, the `.smk` file (or Snakefile); in Nextflow, te `.nf` file; in CWL, `.cwl` file; and in WDL, `.wdl`. However, to ensure reproducibility, it is a good practice to share complete pipelines, meaning folder structures, additional files and software specification, as well all custom scripts developed for the analysis. These files can be shared using the same tools as other forms of code, namely GitHub or any other Git hosting services. Alternatively, they can be uploaded to hosting services specialized in workflows, like WorkflowHub [@https://workflowhub.eu/], currently in beta.

When sharing workflows, consider that sharing software versioning is necessary for your collaborators to reproduce your pipeline using their own computing setup. Conda environments, for example, can be easily created from an environment file (in YAML language), which can be exported from an existing environment. Notably, Snakemake and Nexflow can be configured to automatically build isolated environments for each rule or step, enabling running different versions of a program within the same pipeline (which is especially helpful when needing both Python 2 and 3 in the same pipeline, for example). Besides sharing the specifications of an environment, it is possible to share the environment itself via containers, using platforms like Docker and Singularity as we will discuss in [Level 3](#level-3-community).

### 2.5 Write manuscripts collaboratively

Writing articles is the main way we share our research with the scientific community at large. However, in a highly interdisciplinary field as computational biology, writing manuscripts is also a collaborative effort. The traditional computer tools for writing documents are not suitable for this type of collaboration, resulting in files with different names, jumping from one e-mail inbox to another, and multiple and even contradictory final versions. Let's avoid this by streamlining collaborative manuscript writing with tools made for that purpose.

Big companies have become aware of the need for collaborative writing, developing online applications that can be simultaneously edited by multiple people. Google's Docs and Microsoft's Office 365 are well-known word processors designed for this purpose, where the text is displayed with the exact appearance than in a printout (know as _What-You-See-Is-What-You-Get_ , or WYSIWYG) and the text can be formatted making use of the internal features of the application. The advantage of these technologies is that they are extremely user-friendly and require no additional knowledge. They are a good option when one or more of your collaborators seeks simplicity, but they are not specifically tailored for the needs of scientific writing, such as adding references, equations and figures. Fortunately, third-party companies have developed plugins for these applications to add references to your document. Companies like Authorea, in the other hand, have developed their own online application specifically designed for writing manuscripts. Authorea, in particular, offers templates for different type of research projects and allows you to easily add references using identifiers (DOI, PubMed, etc.). Consider, nonetheless, that some collaborators may not want to adopt a new tool exclusively for writing manuscripts.

In addition to word processors, text editors are a competitive option to write manuscripts when combined with a markup language—a human-readable computer language that uses tags to delineate formatting elements in a document that will be later rendered. Since the formatting process is internally handled by the application, styling elements (headers, text formatting, equations) can be easily written in text, even achieving greater consistency than word processors. Disciplines closely related to computational biology, such as statistics and mathematics, have historically used the markup language LaTeX for writing articles. This language has a specific syntax to write mathematics constructs as simple text, making it a sound choice for papers with lots of equations. To aid collaborative writing, platforms like Overleaf provide online LaTeX editors, supporting features like real-time editing. In addition to LaTeX, an emerging trend in collaborative writing is to use the lightweight markup language Markdown within the GitHub infrastructure. The software Manubot provides a set of functionalities to write scholarly articles within a GitHub repository, leveraging all the advantages of Git version control and the GitHub hosting platform [@doi:10.1371/journal.pcbi.1007128]. For example, it provides cloud storage, version control, and facilitates the maintainers' work by managing updates via pull requests. The GitHub user interface also allows off-line discussions about the manuscript using issues" and task assignment (See level 3 for tips on project management). Manubot, in particular, accepts citations using manuscripts identifiers and automatically renders the article in PDF and HTML formats. As a drawback, it requires technical expertise in Git and familiarity with GitHub; as an upside, its reliable infrastructure scales well to large and open collaborative projects. 
