## Level 1: Personal Research

The computational biology "journey" begins with you—more specifically, with the set of skills, tools and practices that you have in place to conduct your research. Taking the time to optimally establish these building blocks will have high payoffs later, when you need to go back to your research (which will certainly happen). Consider that your most important collaborator is your future self, either of tomorrow or ten years from now. To attend all the fundamental aspects that make up a solid ground to start any computational biology project, we devised a framework involving four main sequential steps (Table 1).

### Step 1: Choose your programming languages

Different programming languages serve distinctive purposes and have different idiosyncrasies. As such, choosing the right programming language for a project depends on your research goals and personal preference or skill. Additionally, communities usually favor the usage and training of some programming languages over others; utilizing such languages may facilitate integrating your work within the existing ecosystem. 

As computational biology becomes a data intensive discipline, interacting with high-performance computing (HPC) operating systems has become a hallmark of this field. HPC infrastructures commonly use Unix/Linux distributions as their operating system. These platforms are operated from the command line using a command line interpreter known as the "Unix shell". There are multiple versions of Unix shells, being Bash one of the most widely adopted. Besides providing an "user interface", the shell is also a scripting language that allows manipulating files and executing programs through "shell scripts". Unix/Linux operating systems have other interesting perks, such as powerful and fast commands for searching words and manipulating files (e.g. `sed`, `grep` or `join`) as well as the language AWK—a standard feature of Unix/Linux operating systems—that can perform quick text processing, including arithmetic operations.

One of the most common task of any computational biologist is data analysis. The job of a data analyst involves data cleaning, exploration, manipulation, and visualization. Nowadays, Python is the most widely used programming language for data analysis worldwide. Computational biology research had followed this trend, making Python one of the most popular languages for data analysis among researchers. Python acquisition has been facilitated by the availability of packages for biological data analysis easily accessible through package managers such as Pip or Conda. Python has also become one of the standard languages for machine learning and deep learning; increasing usage of these tools in biological research will keep propelling Python as a core programming language in computational biology. In addition to Python, R is the other most prominent language in the field. Arguably, one of R main strengths is the wide array of tools for statistical analysis. Also, as R has been around for several decades, a wide array tools have already been written in this language. Of particular interest is the Bioconductor repository, where many gold-standard tools in computational biology have been published. R usage in data science has deeply benefited from the Tidyverse packages, increasing readability and flow of the traditional R syntax for both data manipulation via `dplyr`, and visualization via `ggplot2`.

Oftentimes, computational biologists require coding their own sets of instructions for processing data via scripts or programs. A script can be described as lightweight program developed to tackle a narrow purpose and usually consisting of a single file. They are most likely written in an interpreted programming language instead of a compiled one. These programming languages are quickly to edit and each instruction can be run interactively, at expense of worse computational performance. In computational biology, the current most common multi-purpose scripting languages are Python and R. When working in a HPC, Shell/Bash scripting is also widely used. Perl and Matlab are also a popular language among bioinformatics and systems biology, respectively. A program, in the other hand, is a larger tool that usually combines multiple scripts and works as a "black box" to the user. It is designed to tackle a larger problem and, thus, is more computationally intensive. To make better use of computational resources, programs are preferably written using compiled languages. Several tools designed for high-weight biological data processing have been written in C/C++. However, in recent years, scientist have been turning to Rust because of its speed, safety and friendly community [@doi:10.1038/d41586-020-03382-2]. Programs can also be written in an interpreted language. especially when computational performance is not a concern. Python and R excel in this area due to their versatility and wide array of modules and libraries available.

Biological data processing is rarely a one-step process. To go from raw data to useful insights, several steps need to be taken in a specific order. Moreover, they are often accompanied by a plethora of decisions regarding parameters. Computational biologists have addressed this need by embracing workflow management systems to automate data analysis pipelines. In its most simplified form, a pipeline can be written as a Shell script where a handful of commands are streamlined one after another, using Shell variables and Shell scripting syntax when needed. This approach is suitable for quick single-use applications. However, when the task involves multiples steps, non-linear decision making and internal checks, Shell scripts become cumbersome. For such cases, dedicated bioinformatics workflow systems have been developed. There are multiple bioinformatics workflow managers, but here we will focus on Snakemake and Nextflow. Snakemake is a workflow management system written in Python, which easily allows to incorporate the syntax of the tool with standard Python code. Similarly, Nextflow was build as an extension of the Groovy—a programming language for the Java virtual machine—and can execute any piece of Groovy code. These tools not only streamline data analysis steps, but also have incorporated multiple features over the years, like interacting with job schedulers to be run in HPCs, automated deployment of computational environments (i.e. software and dependencies), and cloud computing support. The Common Workflow Language features another take on data analysis pipelines, delivering a standard for pipeline definition that can be read by different tools.

### Step 2: Define your project structure

After choosing your programming languages and before starting any coding, we advice you to come up with a well-thought-out project structure. This design should be intentional and tailored to the present and future needs of your project—remember to be kind to your future self. A computational biology project requires, at the very least, a folder structure that supports code, data, and documentation. Although tempting, cramming all kind of files in a unique folder is unsustainable. Instead, separate each one on different folders and subfolders if needed. As additional principles consider that documentation is not optional and raw data is immutable. To simplify this process, you can based your project structure on research templates available off-the-rack. For data science projects, the python package Cookiecutter Data Science cut down the effort to the very minimum [@https://drivendata.github.io/cookiecutter-data-science]. Running the package prompts a questionnaire in the terminal where you can input the project name, authors and other basic information. Then, the program generates a folder structure to store data, raw and processed, separated from notebooks and source code, as well as pre-made files for documentation such as a "readme", a docs folder and a license. Similarly, the Reproducible Research Project Initialization offers a template folder structure that can be cloned from a GitHub repository and modified by the user [@https://github.com/Reproducible-Science-Curriculum/rr-init]. Although the later is simpler than the former, both follow an akin philosophy aimed at research correctness and reproducibility [@doi:10.1371/journal.pcbi.1000424]. For workflow automation projects, we advice a similar folder structure. Snakemake recommends to store each workflow in a dedicated folder separated into workflow-related files—the Snakefile, rules and scripts—results and configuration [@https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html]. In all cases, the folder must be initialize as a git repo for version control, which we will discuss in Step 4. 

Beyond files and folders, the set of software and dependencies needed to run the analysis, workflow or program are also part of the project structure itself. The intricacies of software installation and dependency management are not to be underestimated. Fortunately, package and virtual environment managers significantly reduce this burden. A package manager is a system that automates the installation, upgrading, configuration and removing of community-developed programs; a virtual environment manager, in the other hand, is a tool that generates isolated "environments" containing programs and dependencies that are functionally independent from other environments or the default operating system. Once a virtual environment is activated, a package manager can be used to install third-party programs.

We believe that a computational biology project must start with its own virtual environment. The main reason is reproducibility: environments record a project dependencies and can restore them at will so you can reproduce your analysis in any computer. There are multiple options for both package management and virtual environment management systems—some are language-specific; others, language-agnostic. If you are working with Python, you can initialize a Python environment using virtualenv or pipenv (where different Python versions can be installed). Inside the environment, you can use the Python package manager pip to add Python code stored in the Python Package Index (PyPI), GitHub or locally. For the R language, R-specific environments can be created using `renv`. After initializing the environment, packages can be installed via `install.packages` function from the Comprehensive R Archive Network (CRAN) and CRAN-like repositories. R also has BiocManager to install packages from the Bioconductor repository, which contains relevant software for high-throughput genomic sequencing analysis. To fully manage a dependencies beyond installation, RStudio developed the RStudio Package Manager which works with third-party code available in CRAN, Bioconductor, GitHub or locally. A language-agnostic alternative is Conda—an increasingly popular package manager and a virtual environment manager. It supports program installation from the Anaconda repository, which contains the channel Bioconda specifically tailored to bioinformatics software. Also, if Python is installed, Python dependencies can be installed via pip. Conda is particularly helpful when working with third-part code in all sorts of languages—a common predicament for the computational biologist. Conda package and environment manager is included in both Miniconda and Anaconda distributions. The former is a minimal version of Anaconda, containing only Conda, Python, and a few useful packages.

### Step 3: Choose your working set-up

With the foundation in place, the next step is to start coding. However, a more practical question needs to be answer first: where to code. The simplest tool available for this purpose are text editors. Since writing code is ultimately writing text, any tool where characters can be typed fulfills this purpose. However, coding can be streamlined by additional features as those found in code editors—text editors especially developed for writing code. Crucial features to facilitate coding include syntax highlight, indentation or autocompletion. Commonly used desktop editors include Atom, Sublime, Visual Studio Code, and Notepad++ (Windows only), all which a myriad of plugins available to enhance the coding experience. Command line text editors are also suitable options for coding, being Vim and Emacs the most powerful ones. All of these tools share the advantage of being language agnostic, allowing easy switching between languages, especially handy for the polyglot computational biologist. 

In addition to text editors, integrated development environment (IDE) are also popular options for coding. As its essence, IDE are supercharged text editors, i.e. with multiple other features that make writing code easier. The main parts of an IDE are a code editor (with syntax highlight, indentation and suggestions), a debugger, a folder structure, and a way to execute your code (a compiler or interpreter). IDEs are not language agnostic, meaning that they allow to code in one language. The array of features also comes at a cost—IDEs usually use more memory and imply more visual clutter, if that is a concern of yours. For Python, Jupyter Lab, Spyder and JetBrains' PyCharm are popular options, while for R, RStudio is the gold-standard. Notably, the differences between an IDE and a code editor are somewhat blurry, especially when enough plugins have been added to a code editor.

In the latest years, notebooks have acquired relevance in computational biology research. A notebook is an interactive application that combines live code (read-print-eval loop or REPL), narrative, equations and visualizations. Common notebooks use an interpreted languages such as Python or R, and narrative follows markdown syntax. Data analysis greatly benefits from using notebooks instead of plain text editors or even IDEs: the combination of visuals and texts allows researcher to tell compelling stories about their data, and the interactivity of its code enables quick testing of different strategies. Jupyter notebook is a popular web-based interactive notebook developed originally for Python coding, but also accepts R and other programming languages upon installation of their kernels—a computing engine that executes the notebook's live code "under the hood". Jupyter notebook can also be run in the cloud using platforms such as Google CoLab and Amazon WebServices, taking advantage of the current trend of cloud computing. RStudio also allows the generation of R-based notebooks known as R Markdown, which is specially well-suited for generating reports.

### Step 4: Follow coding good practices

After dealing with steps one to three, finally comes writing code. Coding, however, requires good practices to ensure correctness, sustainability and reproducibility for you, your future self, your collaborators (as we will discuss in Level 2) and the whole community (as we will discuss in Level 3). First and foremost, you need to make sure your code works correctly. In computational biology, correctness implies biological and statistical soundness. Both are big topics beyond the scope of this manuscript. To achieve the former, however, a useful approach is to design positive and negative controls in your program, analysis or workflow. In an experiment, a positive control is a control group that is expected to produce results; a negative control is expected to produce no results. The same approach can be applied to computation, using input data whose output is previously known. Biological soundness can also be tested by quickly assessing expected orders of magnitude in both, intermediate and final files.

Beyond the correct functioning of your code, you must pay attention to the way your code looks, also know as "coding style". This includes a series of small and ubiquitous decisions regarding where and how to add comments; indentation and white spaces usage; variable, function and class naming; and overall code organization. It is true that, as in writing, there is a lot of your own personality in the way you code. However, sticking to existing coding style rules facilitates collaborations with your future self and others. Indeed, as we sometimes have trouble reading our own handwriting, we can also struggle reading our own code if we overlook any guidelines. At the very least, your code must display internal consistency. Further, multiple coding style rules have been published, each specific to a certain programming language. Although arbitrary, most of these rules have been developed with readability in mind. A good place to start, however, are style guides from software development teams. Google, for example, has published guidelines for Python, R, Shell, C++, and HTML/CSS [@https://github.com/google/styleguide]. Also, a series of guidelines for the Python programming language have been published with the name of Python Enhancement Proposal (PEP), where the most widely adopted is PEP-8 [@https://www.python.org/dev/peps/pep-0008/]. To aid flagging stylistic errors in your code, tools called "linters" are usually included with code editors and IDEs or provided as plugins.

In the matter of code styling, two topics merit additional attention: variable naming and comments. Variable names should be descriptive enough to convey an idea about the variable, function or class' content and use. The goal is to produce "self-documented" code that reads close to plain English. To do so, use multi-words variable names if necessary. In such cases, the most common conventions include Camel Case, where the second and subsequents words are capitalized ("camelCase"); Pascal Case, where all words are capitalized ("PascalCase"); and Snake Case, where words are separated by underscores ("snake_case"). Notably, all these conventions can be used in a same coding style to differentiate variables, functions and classes. For example, PEP-8 recommends Snake Case for functions and variables, and Pascal Case for class names. In addition to master variable naming, code comments—explanatory human-readable statements not evaluated by the program—are necessary to enhance the code's readability. No matter how beautiful and well-organized your code is, high-level code decisions will not be obvious unless stated. As a corollary, code explanations that can be deduced from the syntax itself should be omitted. Comments can span a single line or several ones, forming a block, and can be found in three strategic parts: at the top of the program file ("header comment"), which contains the author and the date of the code and what it accomplishes; above every function ("function header"), which contains the purpose and behavior of the function; and in line, next to tricky code whose behavior is not obvious or warrant a remark.

When working with a sizable code base, a good practice related is to strive for modularity—splitting your code's functionalities into independent entities known as modules. Modularity enhances code readability and reusability—enabling your code to be used by you or others in future applications—and expedites maintenance. In Python, subdivisions are defined as follow: a module is a collection of functions and global variables, a package a collection of modules, a library a collection of packages, and a framework a collection of libraries. Modules are simply files with the `.py` extension. Packages, in the other hand, must be indicated to the Python interpreter adding a file named `__init__.py` (which could be empty or not). 

Equally as important as writing good code is to use version control—the practice of tracking and managing changes in your code. This is a good and necessary practice even if your only collaborator is your future self. One of the main advantages of version control is keeping a changeling of your files that can be further used to go back to previous versions of your code (especially helpful when new features negatively impact the function of the code base), and remind you of previous approaches disregarded in newer versions. The most widely used version control system, Git, achieves these tasks with the command `git checkout`. Additionally, version control also allows you to safely try new functionalities using "branches"—carbon copies of the main original branch (known as "master") where you can add code independently and optionally merge it to the original one. Git creates branches using the command `git branch` and the same `git checkout` can be used to switch among them. We will discuss the utility and implementation of branches in collaborative projects in the next section (See Level 2: Collaboration). Nowadays, there are multiple code repositories that also provide version control with Git, such as GitHub, GitLab or Bitbucket. They have the additional benefit of backing up your code and code history in the cloud, keeping your work safe and shareable.

**Table 1**: Steps to start a computational biology project.

| Step                                          | Use case: common tools                                       |
| --------------------------------------------- | ------------------------------------------------------------ |
| **Step 1:** Select your programming languages | &bull; _Interacting with a Unix/Linux HPC_: Shell/Bash<br />&bull; _Data analysis:_ Python, R<br />&bull; _Scrips and tool development:_ Python, R, Perl<br />&bull; _Tool development (high-performance):_ C/C++, Rust<br />&bull; _Research pipelines_: Snakemake (Python), Nextflow (Groovy) |
| **Step 2:** Setup your coding environment     | &bull; _Text editors_: Atom, Sublime, Visual Studio Code, Notepad++, Vim, Emacs<br />&bull; _IDEs_: R Studio, Jupyter Lab, Jet Brains (PyCharm), Spyder<br />&bull; _Notebooks_: Jupyter notebooks, R Markdown, Atom/nteract |
| **Step 3:** Define your project structure     | &bull; _Research templates_: Cookiecutter Data Science, Reproducible Research Project Initialization<br />&bull; _Package managers_: pip, Conda<br />&bull; _Virtual environment managers_: Conda, virtualenv, renv |
| **Step 4:** Write high-quality code           | &bull; _Literate programming_: Markdown, R Markdown, Jupyter Notebooks.<br />&bull; _Coding good practices_: commenting (functions, decisions), styling (variable naming, linter, pep8) <br />&bull; _Version control_: Git and Git GUIs (GitHub Desktop, GitKraken), GitHub/GitLab/BitBucket |

**Table 1 (option 2)**: Steps to start a computational biology project.

<table>
    <thead>
        <tr>
            <th>Step</th>
            <th>Use case</th>
            <th>Common tools<</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=5><b>Step 1:</b> Select your programming languages</td>
            <td rowspan=1>Interacting with a Unix/Linux HPC</td>
            <td>&bull; Shell/Bash</td>
        </tr>
        <tr>
            <td rowspan=1>Data analysis</td>
            <td>&bull; Python<br />&bull; R</td>
        </tr>
        <tr>
            <td rowspan=1>Scrips and tool development:</td>
            <td>&bull; Python<br />&bull; R<br />&bull; Perl</td>
        </tr>
        <tr>
            <td rowspan=1>Tool development (high-performance)</td>
            <td>&bull; C/C++<br />&bull; Rust</td>
        </tr>
        <tr>
            <td rowspan=1>Pipelines</td>
            <td>&bull; Snakemake (Python))<br />&bull; Nextflow (Groovy)</td>
        </tr>
    </tbody>
    <tbody>
        <tr>
            <td rowspan=5><b>Step 2:</b> Setup your coding environment</td>
            <td rowspan=1>Text editors</td>
            <td>&bull; Atom<br />&bull; Sublime<br />&bull; Visual Studio Code<br />&bull; Vim<br />&bull; Emacs</td>
        </tr>
        <tr>
            <td rowspan=1>Notebooks</td>
            <td>&bull; Jupyter Notebooks<br />&bull; R Markdown<br />&bull; Atom (nteract)</td>
        </tr>
        <tr>
            <td rowspan=1>IDEs</td>
            <td>&bull; R Studio<br />&bull; Jupyter Lab<br />&bull; PyCharm<br />&bull; Spyder</td>
        </tr>
    </tbody>
</table>

