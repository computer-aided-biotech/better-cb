---
author-meta:
- Daniela C. Soto *
- "Benjam\xEDn J. S\xE1nchez *"
- Megan Y. Dennis
- Nikolaus Sonnenschein
bibliography:
- content/manual-references.json
date-meta: '2022-04-20'
header-includes: "<!--\nManubot generated metadata rendered from header-includes-template.html.\nSuggest improvements at https://github.com/manubot/manubot/blob/master/manubot/process/header-includes-template.html\n-->\n<meta name=\"dc.format\" content=\"text/html\" />\n<meta name=\"dc.title\" content=\"Tools for (better) computational biology\" />\n<meta name=\"citation_title\" content=\"Tools for (better) computational biology\" />\n<meta property=\"og:title\" content=\"Tools for (better) computational biology\" />\n<meta property=\"twitter:title\" content=\"Tools for (better) computational biology\" />\n<meta name=\"dc.date\" content=\"2022-04-20\" />\n<meta name=\"citation_publication_date\" content=\"2022-04-20\" />\n<meta name=\"dc.language\" content=\"en-US\" />\n<meta name=\"citation_language\" content=\"en-US\" />\n<meta name=\"dc.relation.ispartof\" content=\"Manubot\" />\n<meta name=\"dc.publisher\" content=\"Manubot\" />\n<meta name=\"citation_journal_title\" content=\"Manubot\" />\n<meta name=\"citation_technical_report_institution\" content=\"Manubot\" />\n<meta name=\"citation_author\" content=\"Daniela C. Soto *\" />\n<meta name=\"citation_author_institution\" content=\"Genome Center, MIND Institute, and Department of Biochemistry &amp; Molecular Medicine, Davis, CA 95616,USA\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0002-6292-655X\" />\n<meta name=\"twitter:creator\" content=\"@dcsoto_cl\" />\n<meta name=\"citation_author\" content=\"Benjam\xEDn J. S\xE1nchez *\" />\n<meta name=\"citation_author_institution\" content=\"Department of Bioengineering, Technical University of Denmark, Kgs. Lyngby, 2800, Denmark\" />\n<meta name=\"citation_author_orcid\" content=\"0000-0001-6093-4110\" />\n<meta name=\"twitter:creator\" content=\"@BenjaSanchez\" />\n<meta name=\"citation_author\" content=\"Megan Y. Dennis\" />\n<meta name=\"citation_author_institution\" content=\"Genome Center, MIND Institute, and Department of Biochemistry &amp; Molecular Medicine, Davis, CA 95616,USA\" />\n<meta name=\"twitter:creator\" content=\"@meganamsu\" />\n<meta name=\"citation_author\" content=\"Nikolaus Sonnenschein\" />\n<meta name=\"citation_author_institution\" content=\"Department of Bioengineering, Technical University of Denmark, Kgs. Lyngby, 2800, Denmark\" />\n<meta name=\"twitter:creator\" content=\"@phantomas1234\" />\n<link rel=\"canonical\" href=\"https://computer-aided-biotech.github.io/better-cb/\" />\n<meta property=\"og:url\" content=\"https://computer-aided-biotech.github.io/better-cb/\" />\n<meta property=\"twitter:url\" content=\"https://computer-aided-biotech.github.io/better-cb/\" />\n<meta name=\"citation_fulltext_html_url\" content=\"https://computer-aided-biotech.github.io/better-cb/\" />\n<meta name=\"citation_pdf_url\" content=\"https://computer-aided-biotech.github.io/better-cb/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"application/pdf\" href=\"https://computer-aided-biotech.github.io/better-cb/manuscript.pdf\" />\n<link rel=\"alternate\" type=\"text/html\" href=\"https://computer-aided-biotech.github.io/better-cb/v/8ec50eabfd693fc21959bce490f441f7c9d7df8d/\" />\n<meta name=\"manubot_html_url_versioned\" content=\"https://computer-aided-biotech.github.io/better-cb/v/8ec50eabfd693fc21959bce490f441f7c9d7df8d/\" />\n<meta name=\"manubot_pdf_url_versioned\" content=\"https://computer-aided-biotech.github.io/better-cb/v/8ec50eabfd693fc21959bce490f441f7c9d7df8d/manuscript.pdf\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"twitter:card\" content=\"summary_large_image\" />\n<link rel=\"icon\" type=\"image/png\" sizes=\"192x192\" href=\"https://manubot.org/favicon-192x192.png\" />\n<link rel=\"mask-icon\" href=\"https://manubot.org/safari-pinned-tab.svg\" color=\"#ad1457\" />\n<meta name=\"theme-color\" content=\"#ad1457\" />\n<!-- end Manubot generated metadata -->"
keywords:
- markdown
- publishing
- manubot
lang: en-US
manubot-clear-requests-cache: false
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
title: Tools for (better) computational biology
...






<small><em>
This manuscript
([permalink](https://computer-aided-biotech.github.io/better-cb/v/8ec50eabfd693fc21959bce490f441f7c9d7df8d/))
was automatically generated
from [computer-aided-biotech/better-cb@8ec50ea](https://github.com/computer-aided-biotech/better-cb/tree/8ec50eabfd693fc21959bce490f441f7c9d7df8d)
on April 20, 2022.
</em></small>

## Authors



+ **Daniela C. Soto ***<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0002-6292-655X](https://orcid.org/0000-0002-6292-655X)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [dcsoto](https://github.com/dcsoto)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [dcsoto_cl](https://twitter.com/dcsoto_cl)<br>
  <small>
     Genome Center, MIND Institute, and Department of Biochemistry & Molecular Medicine, Davis, CA 95616,USA
  </small>

+ **Benjamín J. Sánchez ***<br>
    ![ORCID icon](images/orcid.svg){.inline_icon}
    [0000-0001-6093-4110](https://orcid.org/0000-0001-6093-4110)
    · ![GitHub icon](images/github.svg){.inline_icon}
    [BenjaSanchez](https://github.com/BenjaSanchez)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [BenjaSanchez](https://twitter.com/BenjaSanchez)<br>
  <small>
     Department of Bioengineering, Technical University of Denmark, Kgs. Lyngby, 2800, Denmark
  </small>

+ **Megan Y. Dennis**<br>
    · ![GitHub icon](images/github.svg){.inline_icon}
    [meganamsu](https://github.com/meganamsu)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [meganamsu](https://twitter.com/meganamsu)<br>
  <small>
     Genome Center, MIND Institute, and Department of Biochemistry & Molecular Medicine, Davis, CA 95616,USA
  </small>

+ **Nikolaus Sonnenschein**<br>
    · ![GitHub icon](images/github.svg){.inline_icon}
    [phantomas1234](https://github.com/phantomas1234)
    · ![Twitter icon](images/twitter.svg){.inline_icon}
    [phantomas1234](https://twitter.com/phantomas1234)<br>
  <small>
     Department of Bioengineering, Technical University of Denmark, Kgs. Lyngby, 2800, Denmark
  </small>



## Abstract{.page_break_before}

As biotechnological and biomedical research are increasingly fed by the insights arising from computation, the conversation about good practices in computational biology becomes more and more prominent. An increasing body of literature has addressed practices for shareable, reproducible, and sustainable computational research, from high-level principles for data and software stewardship to deep dives into version control or software automation. However, implementing these practices relies on incorporating the right tools into our daily routines, considering the type, scope, and stage of the research project. Here we provide a compendium of relevant tools for computational biology research, emphasizing their time and place within a continuum that traverses personal, collaborative, and community practices. This compendium will serve as a starting point and guide to help navigate the ongoing influx of tools and how to best incorporate them into a computational biologist's working routine, enabling reproducible biomedical and biotechnological research in the long term.


## Introduction

Since Margaret Dayhoff pioneered the field of bioinformatics in the sixties, the application of computational tools in the field of biology has vastly grown in scope and impact. At present, biotechnological and biomedical research are routinely fed by the insights arising from novel computational approaches, machine learning algorithms, and mathematical models. The ever-increasing amount of biological data and the exponential growth in computing power will amplify this trend in the years to come.

The use of computing to address biological questions encompasses a wide array of applications usually grouped under the terms "computational biology" and "bioinformatics." Although distinct definitions have been delineated for each one [@https://www.kennedykrieger.org/sites/default/files/library/documents/research/center-labs-cores/bioinformatics/bioinformatics-def.pdf;@pmid:11552348], here we will consider both under the umbrella term “computational biology,” alluding to any application that involves the intersection of computing and biological data. As such, a computational biologist can be a data analyst, a data engineer, a statistician, a mathematical modeler, a software developer, and many other roles. In praxis, the modern computational biologist will be a “scientist of many hats,” taking on several of the duties listed above. But first and foremost, we will consider a computational biologist as a scientist whose ultimate goal is to answer a biological question or address a need in the life sciences by means of computation.

Scientific computing requires following specific principles to enable shareable, reproducible, and sustainable outputs. Computing-heavy disciplines, such as software engineering and business analytics, have adopted protocols addressing the need for collaboration, visualization, project management, and strengthening of online communities. However, as a highly interdisciplinary and evolving field, computational biology has yet to acquire a set of universal "best practices." Since most computational biologists come from diverse backgrounds and rely on self-study rather than formal education [@doi:10.1109/SANER.2018.8330263], the absence of guidelines may lead many computational biologists astray, using methods that hinder reproducibility and collaboration, such as unreproducible computational workflows or closed-source software, retarding biomedical and biotechnological research.

In recent years, this "guidelines gap" has been addressed by the establishment of FAIR principles—Findability, Accessibility, Interoperability, and Reusability—in 2016 [@doi:10.1038/sdata.2016.18]. Originally developed for data stewardship, FAIR principles have been proposed as universal guidelines for all research-related outputs [@doi:10.1016/j.patter.2021.100206]. However, translating these high-level principles into day-to-day practices requires additional nuances based on the type of research, the size and scope of the project, and the researcher's experience. To address the need for FAIR scientific software, for example, the framework ADVerTS (availability of software, documenting software, version control, testing, and support) has been proposed as a set of "barely sufficient" practices [@doi:10.1016/j.patter.2021.100206]. More broadly, reviews exist covering general topics for bench scientists new to computational biology—such as programming and project organization [@doi:10.1371/journal.pcbi.1005510;@doi:10.1371/journal.pbio.1001745;@doi:10.1093/gigascience/giz054;@doi:10.1038/nbt.2740;@doi:10.1371/journal.pcbi.1005871]—to detailed descriptions for the more seasoned data scientist—such as workflow automation [@doi:10.1101/2020.06.30.178673], software library development [@doi:10.1016/j.cels.2017.08.003], software version control with the cloud service GitHub [@doi:10.1371/journal.pcbi.1004947], and interactive data science notebooks with Jupyter [@arxiv:1810.08055v1].

Although the above reviews are immensely helpful, an overview of tools for better computational biology is missing. Indeed, guiding principles and general advice are key to establishing a behavior roadmap but their implementation is enabled by incorporating the right tools into our daily working routine. Tool selection has many components, such as availability, suitability, and personal preference; although the latter is left to the reader, here we will shed light on the first two. We premise that good practices in computational biology lie within a continuum that traverses three levels: personal (you), collaboration (your group), and community (your field) (Figure {@fig:levels-fig}). Each of these levels has a different set of requirements and challenges, as well as a specific set of tools that can be used to address them. Here, we compiled a curated list of these tools, emphasizing their time and place in a computational biology research project. Committed to practicality, we illustrated the utility of these tools in case studies covering a wide spectrum of research topics that computational biologists can use to model their own practices, modifying them to suit their own needs and preferences.

![The three "levels" of computational biology include your personal research, your group and collaborators, and the broader scientific community.](images/Figure1.png){#fig:levels-fig width="75%"}


## Level 1: Personal Research

The computational biology journey begins with you and the set of skills, tools, and practices that you have in place to conduct your research. Taking the time to optimally establish these building blocks will have high payoffs later when you find yourself going back to previous analyses. Consider that your most important collaborator is your future self, be it tomorrow or several years from now. We devised a framework involving four main sequential steps to kickstart any computational biology project (Table @tbl:personal-tools).

| |
| - |

Table: Steps involved in starting a computational biology project. {#tbl:personal-tools}

<table>
    <thead>
        <tr>
            <th>Step</th>
            <th>Use case</th>
            <th>Common tools</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan=5><b>Step 1:</b> Choose your programming languages</td>
            <td rowspan=1>Interacting with a Unix/Linux HPC</td>
          <td>&bull; <b>Shell/Bash</b> [@https://www.gnu.org/software/bash/]</td>
        </tr>
        <tr>
            <td rowspan=1>Data analysis</td>
          <td>&bull; <b>Python</b> [@https://python.org], <b>R</b> [@https:r-project.org]</td>
        </tr>
        <tr>
            <td rowspan=1>Scripts and programs</td>
          <td>&bull; <i>Interpreted</i>: <b>Python</b> [@https://python.org], <b>R</b> [@https://r-project.org], <b>Perl</b> [@https://perl.org], <b>MATLAB<\b> [@https://www.mathworks.com/], <b>Julia<\b> [@https://julialang.org/]<br />&bull; <i>Compiled</i>: <b>C/C++</b> [@https://www.cplusplus.com/], <b>Rust</b> [@https://www.rust-lang.org/]</td>
        </tr>
        <tr>
            <td rowspan=1>Workflows</td>
          <td> &bull; <i>Linux-based</i>: <b>shell script</b>, <b>GNU Make</b> [@https://www.gnu.org/software/make/] <br /> &bull; <i>Workflow management systems</i>: <b>Snakemake</b> (Python) [@https://snakemake.github.io/], <b>Nextflow</b> (Groovy) [@https://www.nextflow.io/], <br />&bull; <i>Worflow specifications:</i> <b>CWL</b> [@https://www.commonwl.org/], <b>WDL</b> [@https://openwdl.org/]</td>
        </tr>
    </tbody>
     <tbody>
        <tr>
            <td rowspan=5><b>Step 2:</b> Define your project structure</td>
            <td rowspan=1>Project structure</td>
          <td>&bull; <i>Templates</i>: <b>Cookiecutter Data Science</b> [@https://drivendata.github.io/cookiecutter-data-science], <b>rr-init</b> [@https://github.com/Reproducible-Science-Curriculum/rr-init] <br />&bull; <i>Workflows</i>: <b>Snakemake worflow template</b> [@https://github.com/snakemake-workflows/snakemake-workflow-template]</td>
        </tr>
        <tr>
            <td rowspan=1>Virtual environment managers</td>
          <td>&bull; <i>Language-specific</i>: <b>virtualenv</b> (Python) [@https://virtualenv.pypa.io/], <b>renv</b> (R) [@https://rstudio.github.io/renv/index.html]<br />&bull; <i>Language agnostic</i>: <b>Conda</b> [@https://docs.conda.io/]</td>
        </tr>
        <tr>
            <td rowspan=1>Package managers</td>
          <td>&bull; <i>Language-specific</i>: <b>pip</b> (Python) [@https://pip.pypa.io/], <b>Bioconductor</b> (R) [@https://www.bioconductor.org/], <b>R Studio package manager</b> (R) [@https://www.rstudio.com/products/package-manager/]<br />&bull; <i>Language-agnostic</i>: <b>Conda</b> [@https://docs.conda.io/]</td>
        </tr>
    </tbody>
    <tbody>
        <tr>
            <td rowspan=5><b>Step 3:</b> Choosing your working set-up</td>
            <td rowspan=1>Text editors</td>
          <td>&bull; <i>Desktop applications</i>: <b>Atom</b> [@https://atom.io/], <b>Sublime</b> [@https://www.sublimetext.com/], <b>Visual Studio Code</b> [@https://code.visualstudio.com/], <b>Notepad++</b> [@https://notepad-plus-plus.org/]<br />&bull; <i>Command line</i>: <b>Vim</b>[@https://www.vim.org/], <b>GNU Emacs</b>[@https://www.gnu.org/software/emacs]</td>
        </tr>
         <tr>
            <td rowspan=1>IDEs</td>
           <td>&bull; <i>For Python</i>: <b>JupyterLab</b> [@https://jupyter.org/], <b>JetBrains/PyCharm</b> [@https://www.jetbrains.com/pycharm/], <b>Spyder</b> [@https://www.spyder-ide.org/]<br />&bull; <i>For R</i>: <b>R Studio</b> [@https://www.rstudio.com/]</td>
        </tr>
        <tr>
            <td rowspan=1>Notebooks</td>
          <td>&bull; <b>Jupyter</b> (Python, R) [@https://jupyter.org/], <b>R Markdown</b> (R) [@https://rmarkdown.rstudio.com]</td>
        </tr>
    </tbody>
    <tbody>
        <tr>
            <td rowspan=5><b>Step 4:</b> Follow good coding practices</td>
            <td rowspan=1>Coding style</td>
          <td>&bull; <i>Styling guides:</i> <b>PEP-8</b> (Python) [@https://www.python.org/dev/peps/pep-0008/], <b>Google</b> (Python, R) [@https://github.com/google/styleguide]<br />&bull; <i>Automatic code formatting:</i> <b>Black</b> (Python) [@https://black.readthedocs.io/en/stable/], <b>Snakefmt</b> (Snakemake) [@https://github.com/snakemake/snakefmt]</td>
        </tr>
        <tr>
            <td rowspan=1>Literate programming</td>
          <td>&bull; <b>Markdown</b> [@https://www.markdownguide.org/] <br />&bull; <b>R Markdown</b> [@https://rmarkdown.rstudio.com]</td>
        </tr>
        <tr>
            <td rowspan=1>Version control</td>
          <td>&bull; <i>Version control system:</i> <b>Git</b> [@https://git-scm.com]<br />&bull; <i>Code repositories:</i> <b>GitHub</b> [@https://github.com], <b>GitLab</b> [@https://gitlab.com], <b>Bitbucket</b> [@https://bitbucket.org]<br />&bull; <i>Git GUIs:</i> <b>GitHub Desktop</b> [@https://desktop.github.com/], <b>GitKraken</b> [@https://www.gitkraken.com/]</td>
        </tr>
    </tbody>
</table>


### Step 1: Choose your programming languages

Different programming languages serve distinctive purposes and have unique idiosyncrasies. As such, choosing a programming language for a specific project depends on your research goals, personal preferences, and skillsets. Additionally, communities usually favor the usage and training of some programming languages over others; utilizing such languages may facilitate integrating your work within the existing ecosystem.

Interacting with high-performance computing (HPC) clusters has become a hallmark for the data-intensive discipline of computational biology. HPC infrastructures commonly use Unix/Linux distributions as their operating system. To interact with these platforms, a command-line interpreter known as the shell must be used. There are multiple versions of shells, with Bash [@https://www.gnu.org/software/bash/] being one of the most widely adopted. In addition to providing an interface, the shell is also a scripting language that allows manipulating files and executing programs through shell scripts. Unix/Linux operating systems have other interesting perks, such as powerful, fast commands for searching and manipulating files (e.g., sed, grep, or join) as well as the language AWK, which can perform quick text processing and arithmetic operations.

One of the most common tasks of any computational biologist is data analysis, which usually involves data cleaning, exploration, manipulation, and visualization. Currently, Python [@https://python.org] is the most widely used programming language for data analysis [@https://insights.stackoverflow.com/survey/2021;@https://www.kaggle.com/kaggle/kaggle-survey-2021]. Python is also a popular language among computational biologists, a trend that will likely continue as machine learning and deep learning are more widely adopted in biological research. Python usage has been facilitated by the availability of packages for biological data analysis accessible through package managers such as pip [@https://pip.pypa.io/] or Conda [@https://docs.conda.io/]. Likewise, R [@https://r-project.org] is another prominent language in the field. Arguably, one of the main strengths of R is its wide array of tools for statistical analysis. Of particular interest is the Bioconductor repository [@https://www.bioconductor.org/], where many gold-standard tools for biological data analysis have been published and can be installed using BiocManager [@https://github.com/Bioconductor/BiocManager]. R usage in data science has deeply benefited from the Tidyverse packages [@doi:10.21105/joss.01686] and surrounding community, increasing the readability of the R syntax for both data manipulation via dplyr and visualization via ggplot2.

Computational biologists often must code their own sets of instructions for processing data using scripts or tools. In computational biology, a script often refers to a lightweight single-file program written in an interpreted programming language and developed to perform a specific task. Scripts are quick to edit and can be run interactively but at the expense of computational performance. To automate instructions in HPC clusters, shell scripts are commonly used. For other purposes, the most widely used scripting languages are Python [@https://python.org] and R [@https://r-project.org], but Perl [@https://perl.org], MATLAB [@https://www.mathworks.com/], and Julia [@https://julialang.org/] are preferred by some researchers for bioinformatics, systems biology, and statistics, respectively. A computational biology tool, on the other hand, is a more complex program designed to tackle computationally intensive problems like developing new algorithms. Several tools devised for data-intensive biology have been written in compiled languages such as C/C++ [@https://www.cplusplus.com/]. In recent years, however, scientists have been turning to Rust [@https://www.rust-lang.org/] due to its speed, memory safety, and active community [@doi:10.1038/d41586-020-03382-2]. When computational performance is less of a concern, Python and R are suitable alternatives for computational biology tool development.

Biological data processing is rarely a one-step process. To go from raw data to useful insights, several steps need to be taken in a specific order, accompanied by a plethora of decisions regarding parameters. Computational biologists have addressed this need by embracing workflow management systems to automate data analysis pipelines. A pipeline can be a shell script where commands are written sequentially, using shell variables and scripting syntax when needed. Although effective, this approach provides little control over the workflow and lacks features to run isolated parts of the pipeline or track changes of input and output files. To overcome these limitations, a shell script can be upgraded using the GNU Make [@https://www.gnu.org/software/make/] program, which was originally designed to automate compilation and installation of software, but is flexible enough to build workflows. More sophisticated bioinformatics workflow managers have also been developed such as Snakemake [@https://snakemake.github.io/] based on Python and Nextflow [@https://www.nextflow.io/] based on Groovy (a programming language for the Java virtual machine). These tools offer support for environment managers and software containers (discussed in [Level 3](#level-3-community)), as well as allow for easy scaling of pipelines to both traditional HPC and modern cloud environments. Alternatively, there are available declarative standards to define workflows in a portable and human-readable manner such as the Common Workflow Language (CWL) [@https://www.commonwl.org/] and Workflow Description Language (WDL, pronounced "widdle") [@https://openwdl.org/], used by the cloud computing platform AnVIL [@https://anvilproject.org/;@doi:10.1016/j.xgen.2021.100085]. Although these are not executable, they can be run in CWL- or DWL-enabled engines such as Cromwell [@https://cromwell.readthedocs.io/en/stable/].

### Step 2: Define your project structure

The next step after choosing your programming languages but before starting coding is to develop an organized project structure. The project design should be intentional and tailored to the present and future needs of your project—remember to be kind to your future self! A computational biology project requires, at the very least, a folder structure that supports code, data, and documentation. Although tempting, cramming various file types into one unique folder is unsustainable. Instead, separate files into different folders and subfolders, if needed. To simplify this process, base your project structure on research templates available off-the-rack. For data science projects, the Python package Cookiecutter Data Science [@https://drivendata.github.io/cookiecutter-data-science] decreases the effort to minimal. Running the package prompts a questionnaire in the terminal where you can input the project name, authors, and other basic information. Then, the program generates a folder structure to store data—raw and processed—separate from notebooks and source code, as well as pre-made files for documentation such as a readme, a docs folder, and a license. Similarly, the Reproducible Research Project Initialization (rr-init) offers a template folder structure that can be cloned from a GitHub repository and modified by the user [@https://github.com/Reproducible-Science-Curriculum/rr-init]. Although rr-init is slightly simpler, both follow an akin philosophy aimed at research correctness and reproducibility [@doi:10.1371/journal.pcbi.1000424]. For workflow automation projects, we advise following the Snakemake workflow template [@https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html;@https://github.com/snakemake-workflows/snakemake-workflow-template], storing each workflow in a dedicated folder divided into subfolders for workflow-related files, results, and configuration. In all cases, the folder must be initialized as a git repo for version control (see Step 4).

The software and dependencies needed to execute a tool or workflow are also part of the project structure itself. The intricacies of software installation and dependency management should not be underestimated. Fortunately, package and virtual environment managers significantly reduce this burden. A package manager is a system that automates the installation, upgrading, configuration, and removal of community-developed programs. A virtual environment manager is a tool that generates isolated environments where programs and dependencies are installed independently from other environments or the default operating system. Once a virtual environment is activated, a package manager can be used to install third-party programs. We believe that every computational biology project must start with its own virtual environment to boost reproducibility: environments save the project's dependencies and can restore them at will so the code can be run on any other computer. There are multiple options for both package and virtual environment management—some language-specific and some language-agnostic. If you are working with Python, you can initialize a Python environment using virtualenv [@https://virtualenv.pypa.io/] (where different Python versions can be installed). Inside the environment, you can use the Python package manager pip  [@https://pip.pypa.io/] to import Python code from the Python Package Index (PyPI) repository, GitHub, or locally. For the R language, R-specific environments can be created using renv [@https://rstudio.github.io/renv/index.html], where packages can be installed via the install.packages function from the Comprehensive R Archive Network (CRAN) and CRAN-like repositories. R also has BiocManager to install packages from the Bioconductor repository, which contains relevant software for high-throughput genomic sequencing analysis. Additionally, RStudio Package Manager [@https://www.rstudio.com/products/package-manager/] works with third-party code available in CRAN, Bioconductor, GitHub, or locally. Conda [@https://docs.conda.io/]—a language-agnostic alternative—supports program installation from the Anaconda repository, which contains the channel Bioconda [@https://bioconda.github.io/] specifically tailored to bioinformatics software. Python dependencies can also be installed via pip inside a Conda environment. Conda is particularly helpful when working with third-party code in various languages—a common predicament in computational biology. The Conda package and environment manager is included in both the Anaconda and Miniconda distributions. The latter is a minimal version of Anaconda, containing only Conda, Python, and a few useful packages.

### Step 3: Choose your working set-up

Before coding, a more practical question needs to be answered first: Where to code? The simplest tools available for this purpose are text editors. Since writing code is ultimately writing text, any tool where characters can be typed fulfills this purpose. However, coding can be streamlined by additional features—including syntax highlight, indentation, and auto-completion—available in code editors such as Atom [@https://atom.io/], Sublime [@https://www.sublimetext.com/], Visual Studio Code [@https://code.visualstudio.com/], and Notepad++ [@https://notepad-plus-plus.org/] (Windows only). Command-line text editors such as Vim [@https://www.vim.org/] and Emacs [@https://www.gnu.org/software/emacs] are also suitable options for coding. These tools share the advantage of being language agnostic, which is handy for the polyglot computational biologist.

In addition to text editors, integrated development environments (IDEs) are also popular options for coding. In their essence, IDEs are supercharged text editors comprising a code editor (with syntax highlight, indentation, and suggestions), a debugger, a folder structure, and a way to execute your code (a compiler or interpreter). Some IDEs are not language-agnostic, often only allowing code in one language. The array of features also comes at a cost—IDEs typically use more memory. For Python, Jupyter Lab [@https://jupyter.org/], Spyder [@https://www.spyder-ide.org/], and PyCharm [@https://www.jetbrains.com/pycharm/] are popular options, while for R, RStudio [@https://www.rstudio.com/] is the gold standard. Notably, the differences between an IDE and a code editor are somewhat blurry, particularly when employing plugins with a code editor.

In recent years, notebooks have acquired relevance in computational biology research. A notebook is an interactive application that combines live code (read-print-eval loop or REPL), narrative, equations, and visualizations, internally stored using a format called JavaScript Object Notation (JSON). Common notebooks use interpreted languages such as Python or R, and narrative usually uses Markdown—a lightweight markup language. Data analysis greatly benefits from using notebooks instead of plain text editors or even IDEs. The combination of visuals and texts allows researchers to tell compelling stories about their data, and the interactivity of its code enables quick testing of different strategies. Jupyter [@https://jupyter.org/] is a popular web-based interactive notebook developed originally for Python coding but also accepts R and other programming languages upon installation of their kernels—the computing engine that executes the notebook's live code under the hood. Jupyter notebook can also be executed in the cloud using platforms such as Google CoLaboratory (CoLab) [@https://colab.research.google.com] and Amazon WebServices, taking advantage of the current trend of cloud computing. In addition, RStudio allows the generation of R-based notebooks known as R Markdown [@https://rmarkdown.rstudio.com], which is especially well suited for generating data analysis reports.

### Step 4: Follow good coding practices

With the foundation in place, the next step is to start writing code. Coding, however, requires good practices to ensure correctness, sustainability, and reproducibility for you, your future self, your collaborators, and the whole community. First and foremost, you need to make sure your code works correctly. In computational biology, correctness implies biological and statistical soundness. Although both are topics beyond the scope of this manuscript, a useful approach to evaluate biological correctness is to design positive and negative controls in your program, analysis, or workflow. In scientific experimentation, a positive control is a control group that is expected to produce results; a negative control is expected to produce no results. The same approach can be applied to computation, using input data whose output is previously known. Biological soundness can also be tested by quickly assessing expected orders of magnitude in both intermediate and final files. These checks can be packaged in unit testing (discussed in [Level 2](#level-2-collaboration)).

In addition to correctly functioning code, code appearance, also known as coding style, is important. Code style includes a series of small, ubiquitous decisions regarding where and how to add comments; indentation and white-space usage; variable, function, and class naming; and overall code organization. Although, as in writing, personality and preference differences dictate how you code, coding style rules facilitate collaboration with your future self and others. Indeed, as we sometimes have trouble reading our own handwriting, we can also struggle reading our own code if we disregard guidelines. At the very least, aim to follow internal consistency in writing code. Even better, consider following any of the multiple published coding-style guides such as those from software development teams. Google, for example, has guidelines for Python, R, Shell, C++, and HTML/CSS [@https://github.com/google/styleguide]. Guidelines for Python are available as part of the Python Enhancement Proposal (PEP), known as PEP 8 [@https://www.python.org/dev/peps/pep-0008/]. To facilitate compliance, tools called linters can be incorporated into most code editors and IDEs to flag stylistic errors in your code based on a given style guide. Furthermore, many editors and tools perform automatic code formatting (e.g., Black [@https://black.readthedocs.io/en/stable/] that formats Python code to be PEP 8 compliant), which can greatly facilitate stylistic coherence in a collaborative project. In the case of Snakemake files, stylistic errors can be flagged using the Snakemake linter, which can be invoked with the command snakemake --lint [@https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html], or automatically corrected with the tool Snakefmt [@https://github.com/snakemake/snakefmt], based on Black.

On the matter of code styling, two topics merit additional attention: variable naming and comments. Variable names should be descriptive enough to convey information about the variable, function, or class content and use. The goal is to produce self-documented code that reads close to plain English. To do so, multi-word variable names should be used if necessary. In such cases, the most common conventions include Camel Case, where the second and subsequent words are capitalized (camelCase); Pascal Case, where all words are capitalized (PascalCase); and Snake Case, where words are separated by underscores (snake_case). Notably, these conventions can be used in the same coding style to differentiate variables, functions, and classes. For example, PEP-8 recommends Snake Case for functions and variables and Pascal Case for class names. As most modern code editors and IDEs provide autocompletion of variable, function, and class names, it is no longer a valid excuse to use cryptic one-character variable names (e.g., x, y, z) to save a few keystrokes.

In addition to mastering variable naming, code comments—explanatory human-readable statements not evaluated by the program—are necessary to enhance the code's readability. No matter how beautiful and well-organized your code is, high-level code decisions will not be obvious unless stated. As a corollary, code explanations that can be deduced from the syntax itself should be omitted. Comments can span a single line or several lines, and can be found in three strategic parts: at the top of the program file (header comment), which describes what the code accomplishes and sometimes the code's author/date; above every function (function header), which contains the purpose and behavior of the function; and in line, next to difficult code with behavior that is not obvious or warrants a remark.

Code-styling rules also apply to data science notebooks. However, when writing notebooks, you must also engage in literate programming—a programming paradigm where the code is accompanied by a human-readable explanation of its logic and purpose. In other words, notebooks must tell a story about the analysis, connecting the dots between the code, the results, and the figures. Human-readable language is often written in Markdown [@https://www.markdownguide.org/] when working in Jupyter, or R Markdown [@https://rmarkdown.rstudio.com] when working in R. Little has been written about good practices for literate programming, but our suggested good practices are to include the purpose and interpretation of results for each section of code.

When working with a sizable codebase, we advise modular programming—the practice of subdividing a computer program into independent and interchangeable sub-programs, each one tackling a specific functionality. Modularity enhances code readability and reusability, as well as expedites testing and maintenance. In practice, modularity can be implemented at different levels, from using functions within a single-file program to separating functionalities into different files in a more complex tool. In Python, subdivisions are defined as follows: modules are a collection of functions and global variables, packages are a collection of modules, libraries are a collection of packages, and frameworks are a collection of libraries. Modules are files with .py extension, while packages are folders that contain several .py files, including one called __init__.py which can be empty or not and allows the Python interpreter to recognize a package.

Finally, there is version control, one of the most important personal practices. Version control entails tracking and managing changes in your code. A popular version-control system is Git [@https://git-scm.com], which requires a folder to be initiated as a Git repository, after which changes to any of the files inside would be tracked. File modifications must be staged (using git add) and then committed (using git commit). The commit will serve as a screenshot of your project at that time and stage, which you can review or recover later (using git checkout). Additionally, version control allows you to sa\fely try new functions in branches (using git branch and git checkout)—independent carbon copies of the main original branch (known as main) that you can optionally merge back to the original copy. Currently, there are multiple hosting services that provide online storage of Git repositories, such as GitHub [@https://github.com], GitLab [@https://gitlab.com], or Bitbucket [@https://bitbucket.org], that users can navigate using the web browser or via a graphic user interface (GUI) such as GitHub Desktop [@https://desktop.github.com/] or GitKraken [@https://www.gitkraken.com/]. These platforms have the additional benefit of backing up your code in the cloud, keeping your work safe and shareable, which is especially relevant for collaboration.


## Level 2: Collaboration

Collaboration is a key aspect of scientific research, but it is especially relevant in computational biology, where interdisciplinary knowledge is often needed. Although collaborators can have a wide range of involvement with your project, here we will consider individuals that share a direct relationship with you and your research. Each type of collaboration requires its own set of good practices, which will be covered in the next paragraphs.

### 2.1 Share code

Sharing code is one of the most common practices in software development, where large teams work together to develop complex functions and scripts. Although computational biology projects are usually not as big, proper sharing code is still essential. Hosting services, such as GitHub [@https://github.com], GitLab [@https://gitlab.com], and Bitbucket [@https://bitbucket.org] (Table @tbl:collaboration-tools), allow for a Git repository to be stored online by creating a copy of the repository known as the remote, which becomes the official version of the repository. The key advantage of using a remote is that there will be no direct interaction between different local copies of the repository, also known as clones; instead, each clone will interact with the remote exclusively, updating only if no conflicts between the two exist. This way, if a collaborator updates the remote repository, other collaborators will not be able to send their changes until they update their local copy.

| Goal | Tools |
| --- | ---------- |
| Share code | &bull; _Hosting services_: **GitHub** [@https://github.com], **GitLab** [@https://gitlab.com], **Bitbucket** [@https://bitbucket.org]. <br />&bull; _Git branching strategies:_ **GitHub flow** [@https://guides.github.com/introduction/flow/].<br />&bull; _Tests:_ correctness (e.g. **pytest** [@https://docs.pytest.org/en/stable/], **testthat** [@https://testthat.r-lib.org/]), style (e.g. **flake8** [@https://flake8.pycqa.org/en/latest/]), vulnerabilities (e.g. **Safety** [@https://pyup.io/safety/]), coverage (e.g. **codecov** [@https://about.codecov.io/]). <br />&bull; _Continous integration:_ **tox** [@https://tox.readthedocs.io/en/latest/], **Travis CI** [@https://travis-ci.com/], **Circle CI** [@https://circleci.com/], **Github Actions** [@https://github.com/features/actions]).<br />&bull; _Code reviews_: **Github** [@https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/reviewing-changes-in-pull-requests], **Crucible** [@https://www.atlassian.com/software/crucible], **Upsource** [@https://www.jetbrains.com/upsource/]. |
| Share data | &bull; _FAIR principles_ [@doi:10.1038/sdata.2016.18]: **FAIRshake** [@https://fairshake.cloud/].<br />&bull; _Tidy data_ [@doi:10.18637/jss.v059.i10]. <br />&bull; _Data version control_ [@https://dvc.org/]. |
| Share data science notebooks | &bull; _Static:_ **GitHub **[@https://github.com], **GitLab **[@https://gitlab.com], **NBviewer** [@https://nbviewer.jupyter.org/]. <br />&bull; _Interactive:_ **Binder** [@https://mybinder.org], **Google CoLab** [@https://colab.research.google.com/].<br />&bull; _Comparative:_ **nbdime** [@https://nbdime.readthedocs.io/en/latest/], **ReviewNB** [@https://www.reviewnb.com/]. |
| Share workflows | • _General hosting services_: <b>GitHub</b> [@https://github.com], <b>GitLab</b> [@https://gitlab.com], <b>Bitbucket</b> [@https://bitbucket.org]. **<br />• _Dedicated workflow repositories:_ **Snakemake Workflow Catalog** [@https://snakemake.github.io/snakemake-workflow-catalog/] , **WorkflowHub** [@https://workflowhub.eu/]. |
| Share manuscripts | &bull; _General-purpose word processors:_ **Google Docs** [@https://www.google.com/docs/about/], **Office 365** [@https://www.microsoft.com/en-us/microsoft-365].<br />&bull; _Scholarly word processors:_ **Authorea** [@https://www.authorea.com/].<br />&bull; _Online applications supporting Markup Languages:_ **Overleaf** (LaTeX) [@https://www.overleaf.com/], **Manubot** (Markdown + GitHub) [@https://manubot.org/]. |

Table: Tools for collaborative research. {#tbl:collaboration-tools}

To guarantee that different collaborators can work simultaneously in the same repository, it is best to implement a branching strategy in the repository (Table @tbl:collaboration-tools). In a small team, the most common strategy is to have a single main branch and generate branches from it that each different developer can work on. Then, whenever the developer is ready, they can request to combine—or merge—the changes from their branch into the main branch. This occurs via a process known as pull request (PR). Once a PR has been opened, collaborators can review, approve, and subsequently merge it into the main branch, preserving the commit history. This branching strategy is sometimes referred to as GitHub Flow [@https://guides.github.com/introduction/flow/] and will suffice for most projects. For more complex branching systems, see [Level 3](#level-3-community).

Using Git hosting services for collaboration has many additional benefits. The commit history both shows what was done at each point in time but also specifies the collaborator who made the changes; this allows users to take responsibility for their changes so that if, for example, a bug was introduced, commands such as git blame can pinpoint the cause. To ensure bugs can be easily tracked, descriptive commit messages that follow a standard are recommended [@https://sparkbox.com/foundry/semantic_commit_messages;@https://www.conventionalcommits.org/en/v1.0.0/]. Git hosting services can be accessed interactively online or from the terminal with tools such as GitHub CLI [@https://cli.github.com/]. Finally, Git hosting services also allow collaborators to open issues [@https://docs.github.com/en/github/managing-your-work-on-github/about-issues] for listing pending tasks and/or asking questions, acting as an open forum for development discussions, which has the advantage of remaining accessible for the future (as opposed to closed email discussions).

Another important concept to consider when developing code, especially with other collaborators, is to develop tests, meaning scripts that will run to find errors in the code (Table @tbl:collaboration-tools). Tests can be executed at different levels, from the individual units/components to the system/software as a whole [@https://www.geeksforgeeks.org/types-software-testing/]. Unit tests, in particular, are used to determine if specific modules/functions work as intended within the codebase so that if later the function grows in scope, its proper basic functioning is ensured. For instance, if a function was defined for adding numbers, a simple test would be to assess if the function outputs 13 when the inputs 6 and 7 are provided. Besides unit tests, computational biology projects can benefit from implementing integration tests to evaluate the correct interaction between different modules and smoke tests to indicate if any core functionality has been impacted. Test runners, such as pytest [@https://docs.pytest.org/en/stable/] for Python and testthat [@https://testthat.r-lib.org/] for R, exist to facilitate incorporating tests to the codebase. It is good practice to develop tests at the same time you develop code, as adding tests a posteriori is significantly harder. It is an even better practice to test every single step of the code (from data loading to figure plotting), a concept known in software development as end-to-end testing [@https://smartbear.com/solutions/end-to-end-testing/].

Going beyond testing correctness, flake8 [@https://flake8.pycqa.org/en/latest/] will test styling preferences (for complying with PEP8), Safety [@https://pyup.io/safety/] will test for vulnerabilities among the software's dependencies, and Codecov [@https://about.codecov.io/] will test coverage, or the percentage of the codebase tested. As a rule of thumb for testing coverage, the more lines of code tested, the more reliable the software will be. Different types of tests can be funneled into a single testing pipeline—in a process known as continuous integration (CI)—that can be tuned to run locally whenever commits are made, or online whenever a pull request is opened and/or merged. When running locally, an environment manager/command-line tool, such as tox [@https://tox.readthedocs.io/en/latest/], can help to ensure all tests are executed under different Python versions. Different tools, such as Travis CI [@https://travis-ci.com/] or Circle CI [@https://circleci.com/], can be used to set up the CI cycle online. More recently, GitHub Actions [@https://github.com/features/actions] was developed to run integrations directly from GitHub.

Having tests is a great way to ensure that code fulfills a certain level of correctness and styling. However, it is no replacement for human assessment to determine if the code is correct, necessary, and useful. Therefore, peer code review is essential whenever developing code in collaboration (Table @tbl:collaboration-tools). While tools, such as Crucible [@https://www.atlassian.com/software/crucible] and Upsource [@https://www.jetbrains.com/upsource/], exist for making in-line reviews of each file, the most common approach is for you and/or others to directly review the code using the online review tools provided by various hosting services. In the case of GitHub [@https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/reviewing-changes-in-pull-requests], this not only allows the reviewer to open a comment in any line of the code, which creates a thread for the original author to reply but also to suggest changes that can be approved or dismissed. Reviewers can assess many features of the code, from functionality to documentation, while also following good practices, such as using constructive phrasing, which is outside of the scope of this review but presented in detail elsewhere [@https://google.github.io/eng-practices/review/reviewer;@https://phauer.com/2018/code-review-guidelines].

### 2.2 Share data

The practices of sharing data are similar to sharing code: we should store our datasets, and any changes to them, in a repository and ensure it complies with standards by testing its quality. However, since data has a more consistent structure than code, often existing in standard formats, we should consider additional criteria when sharing it with collaborators (and later with the community). The main set of guidelines that represent these criteria was outlined in what is known as the FAIR principles [@doi:10.1038/sdata.2016.18]: data should be Findable (easy to locate online); Accessible (easy to access once found); Interoperable (easy to integrate with other data/applications/workflows/etc); and Reusable (presented in a way that allows for others to use it for the same or different purposes). Tools like FAIRshake [@https://fairshake.cloud/] can be used to determine if data fits FAIR criteria.

For making data findable, research repositories such as Zenodo [@https://zenodo.org] and Figshare [@https://figshare.com/about] allow you to assign a digital object identifier (DOI) to any group of files you upload, including data and/or code. Alternatively, regular code repositories like GitHub can be used instead, as you can employ commits and/or releases to identify specific versions of the data, in combination with extensions for Large File Storage (LFS), such as git LFS [@https://git-lfs.github.com/], in the case of data files larger than 100 MB [@https://docs.github.com/en/github/managing-large-files/what-is-my-disk-quota]. GitHub can also integrate with Zenodo to automatically archive repositories and assign them a DOI. A final alternative is the Data Version Control (DVC) initiative [@https://dvc.org/], which is especially useful when performing machine learning, as it can keep track of data, machine learning models, and even scoring metrics.

For making data accessible, we encourage as much as possible to make your repositories open access. In cases in which you or your collaborators prefer some restrictions, you can create guest accounts to provide access to private repositories. For making data interoperable, distinctions between raw and clean data have been made [@doi:10.1371/journal.pcbi.1000424], with raw data being the files that came out of the measuring device, and clean data representing the files that are ready to be used for any computational analysis. An important characteristic that clean data should have is to be tidy, which is reviewed in detail elsewhere [@doi:10.18637/jss.v059.i10]. Finally, for making data reusable, thorough documentation of the data is required, including experimental design, measurement units, and possible sources of error.

### 2.3 Share data science notebooks

Jupyter Notebooks have become a fundamental tool for data analysis, which can be shared with collaborators using either static or interactive options. The former shares computational notebooks as rendered text, written internally in HTML. Static notebooks are a good option when you want to avoid any modifications and can work as an archive of past analyses, although interacting with its content is cumbersome—the file must be downloaded and run in a local Jupyter installation. Git-based code repositories, such as GitHub [@https://github.com] and GitLab [@https://gitlab.com], automatically render notebooks that can be later shared using the repository's URL. To facilitate this process, Project Jupyter provides a web application called NBviewer [@https://nbviewer.org], where you can paste a Jupyter Notebook's URL, publicly hosted in GitHub or elsewhere, and renders the file into a static HTML web page with a stable link.

Interactive notebooks, on the other hand, not only render the file but also allow collaborators to fully interact with it, tinkering with parameters or trying new input data—no installation required. Binder Project [@https://mybinder.org] enables users to fully interact with any notebook within a publicly-hosted Git-based repository via a Jupyter Notebook interface, although changes will not be saved to the original file. The platform supports Python and R, among other languages, and additional packages required to run the analysis need to be specified in a configuration file within the repository. Similarly, Jupyter Notebooks can be run interactively using Google CoLab [@https://colab.research.google.com/] by anyone with a Google account. Notebooks can be updated locally, from any public GitHub repository, or from Google Drive. As an added bonus, Google CoLab notebooks can be edited by multiple developers in real-time. In both cases, the machines provided by these services are comparable to a modern laptop, hence these tools may not be suitable for computing-intensive tasks.

Notebooks should be treated like any other piece of code: updates from different collaborators should be managed with version control in a platform such as GitHub. The problem, however, is that git and other version control systems use line-based differences that are not very well suited for the internal JSON representation of Jupyter notebooks. The extension nbdime [@https://nbdime.readthedocs.io/en/latest/] can be installed locally to enable content-aware diffing and merging. Additionally, NBreview [@https://www.reviewnb.com/] can be integrated with GitHub to enable content-aware diffing, displaying the old and new versions of a notebook in parallel to facilitate code review.

### 2.4 Share computational workflows 

Computational biology projects often demand using multi-step analyses with dozens of third-party software and dependencies. Although these steps can be described in the documentation, complex workflows are better shared as stand-alone code that can be easily run with minimal file manipulation from collaborators. Doing so can safeguard the reproducibility and replicability of the analysis, leading to better science and fewer challenges downstream.

The simplest way to share a pipeline is through a shell script that receives input files via the command line, allowing flexibility to run analyses with different input data; however, shell scripts offer little control over the overall workflow and cannot re-run specific parts of the pipeline. To address these issues, pipelines are better shared using a workflow automation system. Theoretically, all of the instructions regarding the workflow could be written in the main pipeline file: in Snakemake, this would be the .smk file (or Snakefile); in Nextflow, te .nf file; in CWL, the .cwl file; and in WDL, the .wdl. However, to ensure reproducibility, it is a good practice to share complete pipelines, meaning folder structures, additional files, and software specifications, as well as custom scripts developed for the analysis. These files can be shared using the same tools as code, namely GitHub or any other Git hosting service. Alternatively, they can be uploaded to hosting services specialized in workflows, like Snakemake Workflow Catalog [@https://snakemake.github.io/snakemake-workflow-catalog/] or WorkflowHub [@https://workflowhub.eu/], currently in beta.

When sharing workflows, consider that sharing software versioning is necessary for your collaborators to reproduce your pipeline using their own computing setup. Conda environments, for example, can be easily created from an environment file (in YAML language), which can be exported from an existing environment. Notably, Snakemake and Nexflow can be configured to automatically build isolated environments for each rule or step, enabling the running of different versions of a program within the same pipeline, which is especially helpful when using both Python 2 and 3 in the same pipeline, for example. In addition to sharing the specifications of an environment, it is possible to share the environment itself via containers, which we will discuss in Level 3.

### 2.5 Write manuscripts collaboratively

Writing articles is the primary way we share our research with the scientific community at large. However, writing manuscripts collaboratively comes with its challenges when using classical word processing tools, often resulting in files with different names, jumping from one email inbox to another, and contradictory final versions. The tools we suggest will help to avoid these issues. Companies have become aware of the need for collaborative writing, developing online applications that can be simultaneously edited by multiple people. Google Docs [@https://www.google.com/docs/about/] and Microsoft Office 365 [@https://www.microsoft.com/en-us/microsoft-365] are well-known word processors designed for this purpose, with text displayed as it would appear as a printout (known as What-You-See-Is-What-You-Get , or WYSIWYG) and formatting performed using internal features of the application. These platforms are extremely user-friendly and require no specialized knowledge making them a good option when collaborators seek simplicity. Although these applications are not specifically tailored for scientific writing, third-party companies have developed plugins enabling useful features, such as adding scientific references to your document (e.g., Paperpile and Zotero). Companies like Authorea [@https://www.authorea.com/] have developed online applications specifically designed for writing manuscripts that offer templates for different types of research projects and allow easy reference additions using identifiers (DOI, PubMed, etc.).

In addition to word processors, text editors are a viable option to write manuscripts when combined with a markup language—a human-readable computer language that uses tags to delineate formatting elements in a document that will be later rendered. Since the formatting process is internally handled by the application, styling elements (e.g., headers, text formatting, and equations) are easily written in text, achieving greater consistency than word processors. Disciplines closely related to computational biology, such as statistics and mathematics, have historically used the markup language LaTeX for writing articles. This language has simple and specific syntax for mathematical constructs making it a popular choice for papers with many equations. To aid collaborative writing, platforms like Overleaf [@https://www.overleaf.com/] provide online LaTeX editors, supporting features like real-time editing. In addition to LaTeX, an emerging trend in collaborative writing uses the lightweight markup language Markdown within the GitHub infrastructure. The software Manubot [@https://manubot.org/] provides a set of functionalities to write scholarly articles within a GitHub repository, leveraging all the advantages of Git version control and the GitHub hosting platform [@doi:10.1371/journal.pcbi.1007128]. For example, it provides cloud storage and version control. The GitHub user interface also allows offline manuscript discussions using issues and task assignments (see Level 3 for tips on project management). Manubot, in particular, accepts citations using manuscript identifiers and automatically renders the article in PDF, HTML, and Word .doc formats. As a drawback, it requires technical expertise in Git and familiarity with GitHub; as an upside, its reliable infrastructure scales well to large and open collaborative projects. The document you are reading now was fully written using Manubot!


## Level 3: Community

The third and final step of this journey is presenting your research to the community. Your main goal should be to share and maintain an open and reproducible project that can sustain community engagement over time. In this section, we will distinguish three sub-goals to make your research: (1) accessible, (2) reproducible, and (3) sustainable. The latter is especially relevant when your research involves developing code that will be used by others in the future (e.g., a tool or workflow), but we believe that our recommendations are relevant to any computational biology project.

### 3.1. Make your research accessible

Making your research accessible includes ensuring that anyone can access your research long after your paper is published. It is extremely frustrating for any researcher to look for software or a set of scripts from a paper published a few years ago, only to find a “404 error” when accessing the source weblink. Equally frustrating is when authors offer code as "available upon reasonable request,” as this often leads to dead-ends and unavailable code.

There are three main ways to publish accompanying code: the supplementary material of the manuscript, privately-owned domains, or uploaded to public repositories. Publishing code as supplementary material has low accessibility for non-open access papers. Moreover, the code will remain completely static and cannot be updated with new features or to correct errors. Making code available via privately-owned domains lacks sustainability, as it requires maintenance of the domain. Therefore, in addition to providing the code as supplementary material and/or via private domains, we recommend uploading it to public repositories, enabling open access and sustainability over time. There are several hosting services for this purpose [@https://github.com;@https://gitlab.com;@https://bitbucket.org] (Table @tbl:community-tools-1), all equally valid and typically dependent on established practices in your specific field.

| Goal | Tool options | Additional remarks |
|---|------|--------|
| Publish your code | &bull; **GitHub** [@https://github.com]<br/>&bull; **GitLab** [@https://gitlab.com]<br/>&bull; **Bitbucket** [@https://bitbucket.org] | All three options allow you to host your repository online for free. Choose whichever is more common in your own field. |
| Introduce your code | &bull; **README file** [@https://www.makeareadme.com]: First file that shows up in a repository. <br/> | Provide a landing page to any repository with a short overview of the code (installation, usage, acknowledgments, etc). |
| Share your code | &bull; Several licensing options [@https://choosealicense.com/licenses]. | Indicate with a license file what restrictions apply when using your code. If you don't include this, you will loose many users. |
| Archive your code | &bull; **Github Releases** [@https://docs.github.com/github/administering-a-repository/managing-releases-in-a-repository]<br/>&bull; **Zenodo** [@https://zenodo.org]: Provides DOI.<br/>&bull; **figshare** [@https://figshare.com/about]: Provides DOI. | Share progressive stable versions of your code as you develop it. Use **semantic versioning** [@https://semver.org] for assigning standard identifiers to your releases. |
| Publish a tool | &bull; **PyPI** [@https://pypi.org]: Python.<br/>&bull; **CRAN** [@https://cran.r-project.org]: R.<br/>&bull;  **Bioconductor** [@https://www.bioconductor.org]: R.<br/>&bull; **Bioconda** [@https://bioconda.github.io]: Language-agnostic. | Produce a package easy to install and use. Especially useful if you think you could have a userbase that will run the same analysis as you on other datasets and/or conditions. |
| Publish an interactive web app | &bull; **Dash** [@https://plotly.com/dash]: Python.<br/>&bull;  **R-Shiny** [@https://shiny.rstudio.com]: R. | Provide easy and interactive data exploration to your users. Especially useful if you have large datasets that can be explored in different ways. |

Table: Tools for making your research accessible. {#tbl:community-tools-1}

When publishing your code in a public repository, two files are fundamental to include: A readme file and a license. A readme file [@https://www.makeareadme.com] introduces users to the code (Table @tbl:community-tools-1) and should include a description of its main intended use, an overview of the installation, the most commonly-used commands, contact information of the developers, and acknowledgments, if appropriate. We recommend keeping the readme file short and written in a markup language such as Markdown [@https://daringfireball.net/projects/markdown/syntax] or reStructuredText [@https://docutils.sourceforge.io/rst.html] that will render automatically on the repository's landing page, below the repository file structure.

Adding a license to a repository is also a crucial step (Table @tbl:community-tools-1). Licenses indicate how the code can be used: Is it free to use for any application? Can users modify the code as they please? Does it come with a warranty that it will work? Can it be used for profit? If no license information is provided, researchers might assume that the code is free to use but copyright law in fact prohibits use without explicit permission by the copyright holder [@https://opensource.guide/legal/]. Many options exist for licensing code [@https://choosealicense.com/licenses], from permissive licenses that allow any kind of use with few or no conditions, like the Unlicense and MIT licenses, to more restrictive licenses that enforce disclosing the source and requiring that any adaptation of the code uses the same license, like the GNU licenses. When deciding on a license, as a rule of thumb, consider that the more requirements you add, the fewer potential users you will have, but the more credit you will receive when users utilize your code for their own needs. Academic researchers must also consider what open-source licenses their university supports, as in many cases it will be the university that owns the copyrights.

As a computational biologist, you will likely continue lines of work from scripts or software you have already published. For instance, you could improve the performance of a given function or add a new set of features entirely. Therefore, you should not only be interested in making your code accessible but also in having different versions available. Creating and archiving successive releases of your code (Table @tbl:community-tools-1) allows the organization of different versions of your code as you develop them. GitHub Releases [@https://docs.github.com/github/administering-a-repository/managing-releases-in-a-repository] is one way to maintain versions with minimal effort. Research repositories, such as Zenodo [@https://zenodo.org] or Figshare [@https://figshare.com/about], not only store your code, notebooks, and data, but also provide a DOI for each version allowing it to be included as a citation in a manuscript. This is especially useful when the publication is not available yet or the current version of the code differs widely from what was published. Research repositories can be combined with code repositories; for example, GitHub has a Zenodo integration that will trigger a new archived version every time a new version is released. Regardless of the solution, we recommend keeping logical order to the releases, using a standard such as semantic versioning [@https://semver.org].

In most cases, providing your code as an organized set of scripts and/or notebooks is sufficient for anyone to consult if they wish to reproduce and/or re-utilize it. However, if your code might be used routinely by other researchers, for instance for studying other organisms or other experimental conditions, consider packaging your code as a tool (Table @tbl:community-tools-1) and publishing through a software repository such as Bioconda [@https://bioconda.github.io], PyPI [@https://pypi.org] if written for Python, or CRAN [@https://cran.r-project.org] and Bioconductor [@https://www.bioconductor.org] if written for R. These increase your possible userbase, as published packages are searchable and can be installed locally with minimal effort.

To increase the accessibility of results to users, an interactive web app or data dashboard can be developed (Table @tbl:community-tools-1). Such apps allow users to interact with data by displaying different sets of variables or changing parameter settings (e.g., the significance of a statistical test). Common options for this goal are Dash [@https://plotly.com/dash] for Python, R, and Julia, and Shiny [@https://shiny.rstudio.com] for R. Both platforms can include interactive graphics generated with plotly data visualization libraries [@https://plotly.com/graphing-libraries].

### 3.2. Make your research reproducible

In addition to having accessible code/data, you also need to ensure anyone can execute your code and obtain the same results. This is especially relevant in computational biology where users will come from different backgrounds and experience. A cornerstone for reproducibility is documentation that explains how the code functions and how to practically achieve your same results. We have distinguished four levels of documentation [@https://documentation.divio.com/]:

* Tutorials: A group of lessons that teach the reader how to become a user of your code;
* How-to guides: A set of documents that clarify how to solve common problems/tasks;
* Explanations: Discussions that clarify particular topics related to your code;
* References: Technical descriptions of your code's variables/classes/functions.

The extent of required documentation will depend on the number of expected users and, relatedly, can affect how many users you attract. If you foresee that your code has little usability outside of your own research, documenting each function using docstrings [@https://www.geeksforgeeks.org/python-docstrings] might be sufficient. However, if you aim for a broader userbase, you might want to add a tutorial for beginners, how-to guides for frequently used routines, and explanations for clarifying the science behind your code, which can be re-used in a manuscript. To publish comprehensive documentation online, consider using (1) a standard documentation language such as reStructuredText [@https://docutils.sourceforge.io/rst.html] or Markdown [@https://daringfireball.net/projects/markdown/syntax], and (2) a documentation platform such as Readthedocs [@https://readthedocs.org], Gitbook [@https://www.gitbook.com], or Bookdown [@https://bookdown.org/] (Table @tbl:community-tools-2). Alternatively, you can use a service like GitHub Pages [@https://pages.github.com/] to host the documentation on a dedicated website.

| Goal | Tool options | Additional remarks |
|---|-------|------|
| Document your code | &bull; **Readthedocs** [@https://readthedocs.org]: Uses **reStructuredText** [@https://docutils.sourceforge.io/rst.html].<br/>&bull; **Gitbook** [@https://www.gitbook.com]: Uses **Markdown** [@https://daringfireball.net/projects/markdown/syntax].<br />&bull; **Bookdown** [@https://bookdown.org/]: Uses **R Markdown** [@https://rmarkdown.rstudio.com/].<br />&bull; **Github Pages** [@https://pages.github.com]: Separate website. | Comprehensive documentation: from tutorials and how-to guides all the way down to function documentation based on all compiled **docstrings** [@https://www.geeksforgeeks.org/python-docstrings]. |
| Reproducible environments | &bull; **Virtual environment managers**: See Table 1.<br/>&bull; **pip-tools** [@https://github.com/jazzband/pip-tools]: Administer several environments in a single project. | As a recommendation, try having the minimum number of dependencies needed to reproduce your results. |
| Reproducible software | &bull; **Docker** [@https://www.docker.com]<br/>&bull; **Singularity** [@https://sylabs.io] | Package your research as a container ready to run in any computer. |
| Reproducible commands | &bull; **Make** [@https://www.gnu.org/software/make] | Build a program by following a series of steps in a single Makefile. |
| Reproducible workflows | &bull; **Workflow management systems**: See Table 1. | Run a pipeline of commands on NGS data in a reproducible way. |
| Reproducible notebooks | &bull; **Interactive notebooks**: See Table 2. | Make your notebooks interactive and reproducible. |

Table: Tools for making your research reproducible. {#tbl:community-tools-2}

Another key aspect of reproducibility is software and dependencies installation. To facilitate this process, you can (1) provide configuration instructions, (2) share dependencies with a virtual environment manager, or (3) share a runtime environment as a container. When setting up software from instructions, it is necessary to ensure the user follows a series of sequential commands in a specific order. To automate this process, Linux systems provide the tool GNU Make [@https://www.gnu.org/software/make/]. Virtual environment managers handle dependencies and facilitate software installation by building virtual environments from requirements files. To achieve repeatable environments, however, it is recommended to include the specific version of software and libraries, a practice known as dependency pinning. Tools such as pip-tools [@https://github.com/jazzband/pip-tools] allow to define different Python environments for a single project depending on the type of user (e.g., end-user versus developer).

Beyond dependency trackers, we recommend ensuring your tool functions as expected across computing infrastructures, even between two different operating systems (e.g., Mac and Windows). This can be achieved via containerization, also known as lightweight virtualization (Table @tbl:community-tools-2). Containers are standardized software that packages an entire runtime environment, meaning everything needed to run your tool: code, dependencies, system libraries and binaries, environmental variables, settings, etc. Instructions for deploying containers are stored in read-only templates called images. Two free tools available for creating containers from images are Docker [@https://www.docker.com] and Singularity [@https://sylabs.io]. While Docker is the most popular framework for containerization [@https://insights.stackoverflow.com/survey/2021], HPC clusters with shared filesystems favor Singularity due to security issues. In most cases, this is not a problem, since Singularity is compatible with all Docker images.

### 3.3. Make your research sustainable

Now that your research can be accessed and reproduced by anyone, the final step is to sustain this over time—also known as code maintenance. This is especially relevant if you continue to develop tools by integrating new features requested by users, which can foster a strong community over time. However, even in the case in which your research is a self-contained project, it is important to ensure that the user community can contact you, in case bugs are discovered or parts of your code malfunction due to dependency updates (part of the "software rot" phenomenon [@https://www.techopedia.com/definition/22202/software-rot]). In the following section, we review useful techniques for making your code/software/research sustainable over time.

You can employ a variety of tools to communicate with users, depending on the size of your user base and the scope of questions/comments received (Table @tbl:community-tools-3). For smaller projects, a single-channel solution like Gitter [@https://gitter.im] offers a simple way for anyone in the community to ask questions and the developers to answer in threads. For larger projects, however, it could become unmanageable to have all discussions in the same channel, so a multiple-channel solution (i.e., forums), such as Google groups [@https://groups.google.com/forum/m] is better suited. GitHub also allows issues to be opened, where collaborators or users can inform developers about bugs or ask questions. Additionally, GitHub recently introduced discussions [@https://docs.github.com/en/discussions] to maintain questions organized in different threads.

| Goal | Tool options | Additional remarks |
|---|-----|-------|
| Tell users how to contact you | &bull; Specific/shorter questions: **Gitter** [@https://gitter.im].<br/>&bull; Larger issues / how-to's: **Google groups** [@https://groups.google.com/forum/m], **GitHub Discussions** [@https://docs.github.com/en/discussions]. | Provide ways for users to contact you for questions, requests, etc. Remember to visit them periodically! |
| Track to-do's in your research | &bull; **Github Issues** [@https://guides.github.com/features/issues] | Detail specific pending to-do's in your research / allow others to request changes and/or highlight bugs. |
| Encourage user contributions | &bull; **Contribution guidelines** [@https://docs.github.com/github/building-a-strong-community/setting-guidelines-for-repository-contributors]: How to open issues / contribute code.<br/>&bull; **Github Wikis** [@https://docs.github.com/github/building-a-strong-community/about-wikis]: More specific how-to guides. | Provide as much information as you can to guide your users. You can also include administrator guidelines. |
| Foster a respectful community | &bull; Smaller projects: **Contributor Covenant** [@https://www.contributor-covenant.org].<br/>&bull; Larger projects: **Citizen Code of Conduct** [@https://github.com/stumpsyn/policies/blob/master/citizen_code_of_conduct.md]. | Essential when you would like researchers to contribute code. |
| Branch your repo sustainably | &bull; **Gitflow** [@https://nvie.com/posts/a-successful-git-branching-model] | Useful when several developers contribute code to the project. Allows users to get access to stable versions of your research in an ongoing project. |
| Keep track of your issues | &bull; **Kanban flowcharts** [@https://www.atlassian.com/agile/kanban]: **Github Projects** [@https://github.com/features/project-management], **GitKraken Boards** [@https://www.gitkraken.com/boards].<br/>&bull; **Scrum practices** [@https://www.scrum.org/resources/what-is-scrum]: **Zenhub** [@https://www.zenhub.com], **Jira** [@https://www.atlassian.com/software/jira]. | Keep track of your pending tasks in different projects with **Agile** [@https://agilemanifesto.org] software development practices. Especially useful if your research is split in many different repositories, each with multiple features/fixes to do. |
| Automate your repo | &bull; **bump2version** [@https://github.com/c4urself/bump2version]: Easier releasing.<br/>&bull; **Danger-CI** [@https://danger.systems/ruby/]: Easier reviewing. | Do less, script more! |

Table: Tools for making your research sustainable. {#tbl:community-tools-3}

Now that users know where to contact you, ensure you have developed contribution guidelines [@https://docs.github.com/github/building-a-strong-community/setting-guidelines-for-repository-contributors] (Table @tbl:community-tools-3), detailing how users should (1) open issues and (2) contribute with their own code changes via PRs. These guidelines are intended for new users/contributors, so should be written in the style of a how-to guide; however, they may also include additional instructions for the main developers or the administrator of the repository. Alternatively, the detailed guidelines can be included in a supplemental wiki, which hosting services offer as part of the repository [@https://docs.github.com/github/building-a-strong-community/about-wikis]. Equally important is a code of conduct (Table @tbl:community-tools-3), which includes expectations on how users should behave in the repository and consequences when someone does not comply, promoting a respectful community. Several code-of-conduct templates exist, such as the Contributor Covenant [@https://www.contributor-covenant.org] for smaller projects and the Citizen Code of Conduct [@https://github.com/stumpsyn/policies/blob/master/citizen_code_of_conduct.md] for larger projects.

Finally, consistent development and maintenance of your software as it grows in scope and number of users will ensure the sustainability of your project. Tools that aid in this include:

1. Branching System: When many developers are involved in a project, more advanced branching methods, such as GitFlow [@https://nvie.com/posts/a-successful-git-branching-model], ensure that users can access functional versions of your code while you work on it. (Table @tbl:community-tools-3). Briefly, GitFlow includes two branches with an infinite lifetime: the main and the development (often named as devel). New branches will be based on the development branch, leaving the main one for stable versions of the code. Every time the development branch is merged into the main branch, a version release is created.
2. Project Management: Tools exist to track, organize, and prioritize user issues (Table @tbl:community-tools-3), all based on Agile [@https://agilemanifesto.org] principles. The simplest approach is implementing a Kanban board [@https://www.atlassian.com/agile/kanban] (as found in GitHub Projects [@https://github.com/features/project-management] or GitKraken Boards [@https://www.gitkraken.com/boards]), where issues are organized in columns that clearly layout the current state of a given task. For larger projects comprising multiple collaborators and/or repositories, a more structured approach, such as a Scrum framework [@https://www.scrum.org/resources/what-is-scrum] (as implemented by Zenhub [@https://www.zenhub.com] and Jira [@https://www.atlassian.com/software/jira]), allows you to prioritize issues by setting milestones and estimating difficulties.
3. Additional Automation: As your project develops, you will find that many aspects can be automated to improve efficiency. bump2version [@https://github.com/c4urself/bump2version] ensures all sections of your code get updated with the new release. Danger-CI [@https://danger.systems/ruby/] and git hooks [@https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks] ensure contributors comply with certain standards in their pull requests. If you are no longer actively maintaining a project, you can use CI (e.g. GitHub Actions [@https://github.com/features/actions]) to schedule regular tests to discover if your tool/code starts malfunctioning due to software rot and/or dependency issues. Finally, we advise against implementing too many automation tools at the start of a project, but adding them as needed. If you find yourself routinely performing a task, consider automating it.


## Case Studies

We will now exemplify the effective use of the introduced tools by presenting three different computational biology projects from the literature (Figure {@fig:cases-fig}) (two more cases are presented in the [Supplementary Material](#supplementary-material)). Note that our list of projects is not meant to be comprehensive, but rather is intended to be a short overview of how projects in computational biology benefit from robust tools and software development practices. Additionally, it will be evident that there is considerable redundancy in chosen tools across case studies. For instance, all projects include an environment manager such as Conda, and a version control system like Git. This redundancy is intentional as it highlights the ubiquity of some tools.

![Examples of computational biology projects and associated depending on the nature of the research and the number of people involved.](images/Figure2.png){#fig:cases-fig width="100%"}

### Case study 1: Genomic variant detection in a large cohort

The availability and affordability of NGS allow for the routine assessment of dozens to thousands of genomes. Resequencing experiments enable the discovery and genotyping of genomic variation within large cohorts to answer key questions regarding population history and susceptibility to disease. For this example, let's consider a project including whole-genome Illumina sequencing and variant identification in thousands of individuals such as Aganezov et al. [@doi:10.1101/2021.07.12.452063]. Herein, the challenge resides in applying a multi-step variant-calling pipeline on many samples in a reproducible manner.

In this particular project, the authors utilized the AnVIL cloud computing platform [@https://anvilproject.org/;@doi:10.1016/j.xgen.2021.100085], which uses WDL for workflow description. However, if you have access to an HPC cluster, then a project of this nature can be performed using the workflow automation tool Snakemake [@https://snakemake.github.io/], employing Python to parse sample names and perform other data handling operations, and following Snakemake workflow template [@https://github.com/snakemake-workflows/snakemake-workflow-template] for folder structure. A Conda [@https://docs.conda.io/] environment can hold all necessary software since a wide array of software designed for genomic analyses is available via the Bioconda [@https://bioconda.github.io/] repository. Coding the workflow can be done in any text editor that offers easy integration with Git tracking and hosting, such as Visual Studio Code [@https://code.visualstudio.com/]. For code styling, you can run Snakefmt [@https://github.com/snakemake/snakefmt] to follow best practices.

A project of this magnitude usually requires collaborators from other research groups. The pipelines and scripts can be shared using a GitHub repository [@https://github.com]. If privacy is a concern, the repository can be set as private and made public in later stages of the project. To write the manuscript, general-purpose word processors such as Google Docs [@https://www.google.com/docs/about/] would suffice. Considering that these types of data are a valuable resource for the community, FAIR principles [@doi:10.1038/sdata.2016.18] for data sharing should be followed. In addition to uploading the raw data in a public repository like the European Nucleotide Archive (ENA) or the National Center for Biotechnology Information (NCBI), we encourage open sharing of your code and notebooks in a GitHub repository archived in Zenodo [@https://zenodo.org] with a DOI.

### Case study 2: Single-cell (sc)RNA-seq data integration

scRNA-seq is a rapidly evolving technology that has enabled the study of cell heterogeneity and developmental changes of a cell lineage, otherwise intractable with bulk RNA-seq. Current scRNA-seq experiments deliver the transcriptomic profiles of thousands to millions of cells [@doi:10.1038/nprot.2017.149], making them a suitable target for machine- or deep-learning approaches. Among the many challenges imposed by this technology, integration of scRNA-seq datasets is key, especially in case-control studies where cell types should be functionally matched across datasets before evaluating differences across conditions. For this case study, we will consider the development of an unsupervised deep-learning method for data integration as described in Johansen and Quon [@doi:10.1186/s13059-019-1766-4].

This kind of project often uses a combination of Python, R, and shell scripting. Python can be used to write and train deep-learning models with TensorFlow [@https://www.tensorflow.org/] or PyTorch [@https://pytorch.org/] libraries. R enables straightforward data pre-processing with tools such as Seurat [@https://satijalab.org/seurat/;@doi:10.1016/j.cell.2021.04.048]. Shell scripting can process large-scale raw data files in HPC clusters. Additionally, we advise using Python’s reticulate library [@https://rstudio.github.io/reticulate/] to incorporate Python tools into the existing R ecosystem. To set up your working directory, we recommend a structure like Cookiecutter Data Science [@https://drivendata.github.io/cookiecutter-data-science], which includes separate folders for trained models and other components of a deep-learning project. To establish a software environment, Python virtual environments, such as virtualenv [@https://virtualenv.pypa.io/], work well with Tensorflow and PyTorch. Coding can be performed in any general-purpose text editor, such as Visual Studio Code [@https://code.visualstudio.com/], where updates can be easily pushed/pulled to/from GitHub. As a good practice, maintain modular, properly-commented code and name files with data stamps and model parameters to facilitate revisiting projects. Additionally, take advantage of tools such as TensorBoard [@https://www.tensorflow.org/tensorboard] to diagnose, visualize, and experiment with your models.

When working with collaborators, code should be shared through a Git hosting service like GitHub. When multiple users need to edit the code in real-time, Google CoLab [@https://colab.research.google.com] offers interactive coding and GPU access. In addition to the code repository, a Manubot [@https://manubot.org/] can be created to write the manuscript collaboratively. To make your tool accessible to a larger community, publish it to a public GitHub and include a readme [@https://www.makeareadme.com] and an appropriate license file [@https://choosealicense.com/licenses]. Considering that most users in the field use R, you can go one step further and share your code as a Bioconductor package [@https://www.bioconductor.org/], making sure your method can be called directly in R and that interacts with standard data structures in the field. For better reproducibility, document your method including example tutorials in a platform like ReadTheDocs [@https://readthedocs.org], and share the software environment needed to deploy the models as a Docker image [@https://www.docker.com]. GitHub issues [@https://guides.github.com/features/issues] and Bioconductor forums [@https://support.bioconductor.org/] are suitable platforms to promptly reply to users’ questions, bug reports, and requests for code enhancements.

### Case study 3: Tool development for constraint-based modeling

The last case study we will present is related to constraint-based modeling, a common approach used for simulating cellular metabolism. In this approach, the metabolic network of a given organism is inferred from its genome and/or literature and converted to a matrix that contains the reaction's stoichiometry. Using a few simple assumptions, this matrix can be used to perform simulations under different experimental conditions to obtain additional insight into cellular physiology [@doi:10.1038/nrg3643]. Several tools have been developed for working with these types of models. Here, we will consider cobrapy [@doi:10.1186/1752-0509-7-74], a community tool for reading/writing constrained-based models and performing basic simulation operations. 

A tool of this nature is especially useful if developed in Python, as it should ideally be presented as a package that can be easily installed with pip [@https://pip.pypa.io/]. The use of an IDE is ideal for this case, as it will provide additional features for testing changes in the tool. Practices that for other case studies were useful now become essential, like complying with coding style and using version control, as hundreds of people will likely read your code. Furthermore, the code should be (1) available via a hosting service such as GitHub [@https://github.com], (2) tested with a continuous development tool such as GitHub Actions [@https://github.com/features/actions], (3) manually reviewed by collaborators to ensure correctness, (4) released following semantic versioning standards  [@https://semver.org], and (5) documented with a companion documentation website, rich with tutorials and how-to guides. As a branching strategy, Gitflow [@https://nvie.com/posts/a-successful-git-branching-model] is probably the best suited, as it allows all changes to existing code in a development branch and stable releases in the main branch.

Finally, due to the large scope of this project, additional considerations must be made to maintain a healthy user base. Offer a place for users to raise questions, such as Gitter [@https://gitter.im], Google groups  [@https://groups.google.com/forum/m], or GitHub Discussions [@https://docs.github.com/en/discussions], and make sure to reply to new questions often. Guidelines should also be provided for everything, including how to: open issues with example templates, contribute using pull-request templates, communicate within the community via a code of conduct, and perform other routine tasks with development guidelines and/or wikis. Addressing issues routinely and quickly is also essential in a project of this nature to avoid giving the impression of a stagnant project. Additional tools such as a Kanban flowchart with the help of GitHub Projects [@https://github.com/features/project-management] will help prioritize issues, or Jira [@https://www.atlassian.com/software/jira] or Zenhub [@https://www.zenhub.com] if several repositories require joint coordination.



## Final words

Good practices in computational biology have gained the spotlight among researchers thanks to several guiding principles published, as well as the increasing usage of Git-based repositories and workflow managers. This review adds to the existing literature by introducing a comprehensive list of good practices and associated tools that can be applied to any computational biology project, regardless of the specific subfield or the experience of the researcher.

We are aware that the many tools and practices introduced in this study and their ever-changing nature may seem overwhelming, especially for someone new to the field. To overcome this, we encourage you to implement only a few practices and tools first, starting from your personal research, and expanding your repertoire over time. More important than any specific tool is keeping a mindset of striving for reproducibility. We also note that our highlighted list of tools is not comprehensive, with many new tools being released. Updated reviews will be essential to help new computational biologists enter the field as well as to keep experienced computational biologists up to date with the latest trends.

The consequences of not following good computational practices are often not seen immediately but become evident and detrimental towards project progress over time. As with all scientific endeavors, computational biology heavily relies on previous knowledge; as such, the good practices we adopt serve as building blocks for the overall reproducibility of the field, propelling novel and exciting future discoveries.


## Acknowledgments

We would like to thank Nelson Johansen for his insights on the scRNA-seq data integration case study.



## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>


## Supplementary Material

### Additional case study 1: RNA-seq differential gene expression

Differential gene expression (DGE) analysis is commonly used in the field of functional genomics. Its main goal is to determine quantitative changes in gene expression levels between different experimental conditions or different populations. Nowadays, given the availability of NGS technologies, most DGE analyses are based on RNA-seq data, being the primary application of the technology [@doi:10.1038/s41576-019-0150-2]. Here, we will consider a study designed to gain insights regarding a specific condition of interest, leading to interest genes that can be functionally characterized in animal models afterward. The experimental setup included a control group and an experimental group with the condition of interest, both sequenced using RNA-seq. The experiment was conducted four times independently to get four replicates per group.

In this example, the most relevant part is personal research. The first step is to decide which programming languages to use. Considering that the bioinformatics analysis will include a first step performed in the command line, where raw reads will undergo quality control and the number of reads per transcripts will be counted, followed by a second step for data filtering and statistical analysis, the programming languages to use will be Shell and R. The folder structure must have separated spaces for raw data, results, documentation, and scripts used in the analysis. We recommend cloning the RR-init template into your HPC cluster. For this project, the best option will be to use a Conda environment with R installed, where you can download packages from Bioconductor and Bioconda. A shell script will be a suitable option for the first part of the analysis in the HPC cluster, and R Studio for the second part of analysis and visualization. We advise to follow literate programming, especially when writing R code, and to track changes using Git.

### Additional case study 2: Genome-scale metabolic model

A sub-group of constraint-based models ([Case Study 3](#case-study-3-tool-development-for-constraint-based-modeling)) are genome-scale metabolic models, where the model represents the complete metabolism of a cell, inferred from genome sequencing. These models are significantly larger in size, making the model generation and curation steps hard to trace back if we don't use adequate tools.  As a reference paper, consider the generation, curation, and validation of a genome-scale model for _Parageobacillus thermoglucosidasius_ [@doi:10.1101/2021.02.01.429138], a thermophilic facultative anaerobic bacteria with promising traits for industrial metabolic engineering.

The first step in a project of this nature is to use one of the many reconstruction algorithms available [@doi:10.1186/s13059-019-1769-1] to start from what is referred to as a draft reconstruction; therefore, the choice of programming language for that section will depend on the selected algorithm. After that, there is a lengthy step of model curation and gap filling, in order to end up with a model that can produce all necessary building blocks of the cell. For this step, we recommend a basic setup of Python as programming language and Conda as environment manager, due to their ease of use and the growing number of Python packages being developed in the field. Additionally, we advise using Jupyter Notebook as the main working setup and Git for version control, as you can use different notebooks (or different versions of a notebook) as logs of analysis performed on your working draft. When collaborating, tools that will be especially useful while working on a genome-scale model are unit-testing, to ensure your model maintains a certain quality as you and others develop it [@doi:10.1038/s41587-020-0446-y], and ReviewNB, to keep track of changes in notebooks across commits and/or branches. Finally, when sharing the model within the community, Zenodo is a great option for defining different versions of the model, and any issue tracker will make it possible for users to pinpoint mathematical or biological inconsistencies in the network.
